{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder  PSRT\n",
      "data  Yau2020_record_processed\n",
      "k  4\n",
      "facet\n",
      "0\n",
      "Overall average metrics across seeds:\n",
      "accuracy: 0.9131424375917768\n",
      "balanced_accuracy: 0.8867418771768333\n",
      "precision_macro: 0.9153044040334631\n",
      "recall_macro: 0.8867418771768333\n",
      "f1_macro: 0.8923677004592127\n",
      "roc_auc_macro: 0.942555741862991\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "# testing the folds\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    'PSRT',\n",
    "]\n",
    "\n",
    "csv_names = [\n",
    "    'Yau2020_record_processed',\n",
    "]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 0. Seed Setting (Optional)\n",
    "###############################################################################\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def find_labels_with_min_count(labels, min_count=15):\n",
    "    \"\"\"\n",
    "    Find the positions of labels whose count is greater than or equal to `min_count`.\n",
    "\n",
    "    Args:\n",
    "        labels (list): A list of labels.\n",
    "        min_count (int): The minimum count to filter labels (default=15).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of labels that meet the count condition.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Find labels with count >= min_count\n",
    "    valid_labels = {label for label, count in label_counts.items() if count >= min_count}\n",
    "    \n",
    "    # Find indices of valid labels\n",
    "    indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_classification_metrics(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Compute multiple classification metrics given true and predicted labels.\n",
    "    Returns a dictionary with:\n",
    "        - accuracy\n",
    "        - balanced_accuracy\n",
    "        - precision\n",
    "        - recall\n",
    "        - f1\n",
    "        - roc_auc (if multi-class or binary)\n",
    "\n",
    "    Note: For ROC-AUC with discrete predictions, we do a one-hot approach with\n",
    "    'ovr' to handle multi-class. If there's only one class in y_true, ROC-AUC is None.\n",
    "    \"\"\"\n",
    "    metrics_dict = {}\n",
    "\n",
    "    # Basic stats\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    metrics_dict['accuracy'] = acc\n",
    "    metrics_dict['balanced_accuracy'] = bal_acc\n",
    "    metrics_dict[f'precision_{average}'] = prec\n",
    "    metrics_dict[f'recall_{average}'] = rec\n",
    "    metrics_dict[f'f1_{average}'] = f1\n",
    "\n",
    "    # Attempt a multi-class or binary ROC-AUC with discrete predictions\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    try:\n",
    "        y_true_1hot = np.zeros((len(y_true), num_classes), dtype=int)\n",
    "        y_pred_1hot = np.zeros((len(y_pred), num_classes), dtype=int)\n",
    "        for i, lbl in enumerate(y_true):\n",
    "            y_true_1hot[i, lbl] = 1\n",
    "        for i, lbl in enumerate(y_pred):\n",
    "            y_pred_1hot[i, lbl] = 1\n",
    "\n",
    "        roc_val = roc_auc_score(\n",
    "            y_true_1hot,\n",
    "            y_pred_1hot,\n",
    "            multi_class='ovr',\n",
    "            average=average\n",
    "        )\n",
    "        metrics_dict[f'roc_auc_{average}'] = roc_val\n",
    "    except ValueError:\n",
    "        # e.g., if y_true has only one class\n",
    "        metrics_dict[f'roc_auc_{average}'] = None\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def five_fold_5nn(distance_matrix, labels, n_neighbors=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform 5-fold stratified cross-validation using a distance matrix and\n",
    "    5-NN classification. Computes multiple metrics on each fold and returns\n",
    "    their average.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): NxN precomputed distances.\n",
    "        labels (array-like): length N array of class labels.\n",
    "        n_neighbors (int): k in k-NN (default=5).\n",
    "        random_state (int): seed for StratifiedKFold reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of average metrics (accuracy, balanced_accuracy,\n",
    "              precision_weighted, recall_weighted, f1_weighted, roc_auc_weighted).\n",
    "    \"\"\"\n",
    "    distance_matrix = np.asarray(distance_matrix)\n",
    "    labels = np.asarray(labels)\n",
    "    N = distance_matrix.shape[0]\n",
    "    assert distance_matrix.shape == (N, N), \"distance_matrix must be NxN\"\n",
    "    assert len(labels) == N, \"labels length must match distance_matrix size\"\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # We'll accumulate metrics over the 5 folds\n",
    "    metrics_list = []\n",
    "    for train_idx, test_idx in skf.split(X=labels, y=labels):\n",
    "        fold_preds = []\n",
    "        fold_true = []\n",
    "\n",
    "        for test_sample in test_idx:\n",
    "            # Distances from test_sample to all others\n",
    "            dist_row = distance_matrix[test_sample].copy()\n",
    "            # Exclude the sample itself\n",
    "            dist_row[test_sample] = np.inf\n",
    "\n",
    "            # Only consider training set distances\n",
    "            train_distances = dist_row[train_idx]\n",
    "\n",
    "            # Indices in 'train_idx' of the n_neighbors nearest neighbors\n",
    "            nn_in_train = np.argpartition(train_distances, n_neighbors)[:n_neighbors]\n",
    "            nn_global_idx = train_idx[nn_in_train]\n",
    "\n",
    "            # Majority vote\n",
    "            neighbor_labels = labels[nn_global_idx]\n",
    "            chosen_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "\n",
    "            fold_preds.append(chosen_label)\n",
    "            fold_true.append(labels[test_sample])\n",
    "\n",
    "        # Compute metrics for this fold\n",
    "        fold_metrics = compute_classification_metrics(fold_true, fold_preds, average='macro')\n",
    "        metrics_list.append(fold_metrics)\n",
    "        \n",
    "        # # ðŸ“¤ Print metrics immediately\n",
    "        # print(f\"Fold {len(metrics_list)} results:\")\n",
    "        # for metric, value in fold_metrics.items():\n",
    "        #     print(f\"  {metric}: {value:.4f}\" if value is not None else f\"  {metric}: None\")\n",
    "\n",
    "    # Average metrics over folds\n",
    "    avg_metrics = {}\n",
    "    # We know the keys from one fold's result\n",
    "    keys = metrics_list[0].keys()\n",
    "    for k in keys:\n",
    "        # Some might be None if there's an issue with ROC-AUC\n",
    "        vals = [m[k] for m in metrics_list if m[k] is not None]\n",
    "        if len(vals) > 0:\n",
    "            avg_metrics[k] = np.mean(vals)\n",
    "        else:\n",
    "            avg_metrics[k] = None\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "def multiple_seeds_5nn(distance_matrix, labels, seeds, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Run the 5-fold 5-NN classification for multiple random_state seeds,\n",
    "    then average the metrics across all runs.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): NxN precomputed distances.\n",
    "        labels (array-like): length N array of class labels.\n",
    "        seeds (list of ints): A list of random seeds to test.\n",
    "        n_neighbors (int): k in k-NN (default=5).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of overall average metrics across all seeds.\n",
    "    \"\"\"\n",
    "    all_runs_metrics = []\n",
    "    for seed in seeds:\n",
    "        run_metrics = five_fold_5nn(distance_matrix, labels,\n",
    "                                    n_neighbors=n_neighbors,\n",
    "                                    random_state=seed)\n",
    "        all_runs_metrics.append(run_metrics)\n",
    "\n",
    "    # Average across seeds\n",
    "    avg_overall = {}\n",
    "    # we'll look at keys from the first run\n",
    "    keys = all_runs_metrics[0].keys()\n",
    "    for k in keys:\n",
    "        vals = [run[k] for run in all_runs_metrics if run[k] is not None]\n",
    "        if len(vals) > 0:\n",
    "            avg_overall[k] = np.mean(vals)\n",
    "        else:\n",
    "            avg_overall[k] = None\n",
    "\n",
    "    return avg_overall\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Configuration\n",
    "# -------------------------------\n",
    "\n",
    "features_needed = [('facet', 0)]\n",
    "\n",
    "###############################################################################\n",
    "# 5. Main\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # (Optional) Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Example: Suppose you have data in a CSV and features in a .npy file\n",
    "    # similar to your Siamese approach\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    for encoder in encoders:\n",
    "        print(\"encoder \", encoder)\n",
    "        for data_name in csv_names:\n",
    "            print(\"data \", data_name)\n",
    "            for k in range(4,5):\n",
    "                print(\"k \", k)\n",
    "\n",
    "                for feature, max_dim in features_needed:\n",
    "                    print(feature)\n",
    "                    print(max_dim)\n",
    "                    path_to_data = f'data/{data_name}'\n",
    "                    df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "                    x_list = df['Accession (version)'].to_list()\n",
    "                    y_list = df['Family'].to_list()\n",
    "                    x_list, y_list = zip(*sorted(zip(x_list, y_list)))\n",
    "                    x_list = list(x_list)\n",
    "                    y_list = list(y_list)\n",
    "                    indices = find_labels_with_min_count(y_list, min_count=15)\n",
    "                    x_list = [x_list[i] for i in indices]\n",
    "                    y_list = [y_list[i] for i in indices]\n",
    "\n",
    "\n",
    "                    path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # for d in range(max_dim + 1):\n",
    "                    # print(d)\n",
    "                    distance_matrix = np.load(path_to_features+f'/k{k}_distance_{feature}{max_dim}.npy')\n",
    "                    subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    seeds = np.arange(1,31)\n",
    "                    le = LabelEncoder()\n",
    "                    y_int = le.fit_transform(y_list)  # shape (N,)\n",
    "                    y_np = np.array(y_int, dtype=np.int32)\n",
    "                    labels = y_np\n",
    "                    results = multiple_seeds_5nn(subD, labels, seeds, n_neighbors=5)\n",
    "                    print(\"Overall average metrics across seeds:\")\n",
    "                    for metric_name, val in results.items():\n",
    "                        print(f\"{metric_name}: {val}\")\n",
    "\n",
    "\n",
    "\n",
    "                    print(\"=================================================================================\")\n",
    "                    print(\"=================================================================================\")\n",
    "                    print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder  PSRT\n",
      "data  Yau2022_record_processed\n",
      "k  4\n",
      "facet\n",
      "0\n",
      "Overall average metrics across seeds:\n",
      "accuracy: 0.9024065372651517\n",
      "balanced_accuracy: 0.8193924931401959\n",
      "precision_macro: 0.8585754342317297\n",
      "recall_macro: 0.8193924931401959\n",
      "f1_macro: 0.8243980681502112\n",
      "roc_auc_macro: 0.9090338370862462\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "# testing the folds\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    'PSRT',\n",
    "]\n",
    "\n",
    "csv_names = [\n",
    "    'Yau2022_record_processed',\n",
    "]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 0. Seed Setting (Optional)\n",
    "###############################################################################\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def find_labels_with_min_count(labels, min_count=15):\n",
    "    \"\"\"\n",
    "    Find the positions of labels whose count is greater than or equal to `min_count`.\n",
    "\n",
    "    Args:\n",
    "        labels (list): A list of labels.\n",
    "        min_count (int): The minimum count to filter labels (default=15).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of labels that meet the count condition.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Find labels with count >= min_count\n",
    "    valid_labels = {label for label, count in label_counts.items() if count >= min_count}\n",
    "    \n",
    "    # Find indices of valid labels\n",
    "    indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_classification_metrics(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Compute multiple classification metrics given true and predicted labels.\n",
    "    Returns a dictionary with:\n",
    "        - accuracy\n",
    "        - balanced_accuracy\n",
    "        - precision\n",
    "        - recall\n",
    "        - f1\n",
    "        - roc_auc (if multi-class or binary)\n",
    "\n",
    "    Note: For ROC-AUC with discrete predictions, we do a one-hot approach with\n",
    "    'ovr' to handle multi-class. If there's only one class in y_true, ROC-AUC is None.\n",
    "    \"\"\"\n",
    "    metrics_dict = {}\n",
    "\n",
    "    # Basic stats\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    metrics_dict['accuracy'] = acc\n",
    "    metrics_dict['balanced_accuracy'] = bal_acc\n",
    "    metrics_dict[f'precision_{average}'] = prec\n",
    "    metrics_dict[f'recall_{average}'] = rec\n",
    "    metrics_dict[f'f1_{average}'] = f1\n",
    "\n",
    "    # Attempt a multi-class or binary ROC-AUC with discrete predictions\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    try:\n",
    "        y_true_1hot = np.zeros((len(y_true), num_classes), dtype=int)\n",
    "        y_pred_1hot = np.zeros((len(y_pred), num_classes), dtype=int)\n",
    "        for i, lbl in enumerate(y_true):\n",
    "            y_true_1hot[i, lbl] = 1\n",
    "        for i, lbl in enumerate(y_pred):\n",
    "            y_pred_1hot[i, lbl] = 1\n",
    "\n",
    "        roc_val = roc_auc_score(\n",
    "            y_true_1hot,\n",
    "            y_pred_1hot,\n",
    "            multi_class='ovr',\n",
    "            average=average\n",
    "        )\n",
    "        metrics_dict[f'roc_auc_{average}'] = roc_val\n",
    "    except ValueError:\n",
    "        # e.g., if y_true has only one class\n",
    "        metrics_dict[f'roc_auc_{average}'] = None\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def five_fold_5nn(distance_matrix, labels, n_neighbors=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform 5-fold stratified cross-validation using a distance matrix and\n",
    "    5-NN classification. Computes multiple metrics on each fold and returns\n",
    "    their average.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): NxN precomputed distances.\n",
    "        labels (array-like): length N array of class labels.\n",
    "        n_neighbors (int): k in k-NN (default=5).\n",
    "        random_state (int): seed for StratifiedKFold reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of average metrics (accuracy, balanced_accuracy,\n",
    "              precision_weighted, recall_weighted, f1_weighted, roc_auc_weighted).\n",
    "    \"\"\"\n",
    "    distance_matrix = np.asarray(distance_matrix)\n",
    "    labels = np.asarray(labels)\n",
    "    N = distance_matrix.shape[0]\n",
    "    assert distance_matrix.shape == (N, N), \"distance_matrix must be NxN\"\n",
    "    assert len(labels) == N, \"labels length must match distance_matrix size\"\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # We'll accumulate metrics over the 5 folds\n",
    "    metrics_list = []\n",
    "    for train_idx, test_idx in skf.split(X=labels, y=labels):\n",
    "        fold_preds = []\n",
    "        fold_true = []\n",
    "\n",
    "        for test_sample in test_idx:\n",
    "            # Distances from test_sample to all others\n",
    "            dist_row = distance_matrix[test_sample].copy()\n",
    "            # Exclude the sample itself\n",
    "            dist_row[test_sample] = np.inf\n",
    "\n",
    "            # Only consider training set distances\n",
    "            train_distances = dist_row[train_idx]\n",
    "\n",
    "            # Indices in 'train_idx' of the n_neighbors nearest neighbors\n",
    "            nn_in_train = np.argpartition(train_distances, n_neighbors)[:n_neighbors]\n",
    "            nn_global_idx = train_idx[nn_in_train]\n",
    "\n",
    "            # Majority vote\n",
    "            neighbor_labels = labels[nn_global_idx]\n",
    "            chosen_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "\n",
    "            fold_preds.append(chosen_label)\n",
    "            fold_true.append(labels[test_sample])\n",
    "\n",
    "        # Compute metrics for this fold\n",
    "        fold_metrics = compute_classification_metrics(fold_true, fold_preds, average='macro')\n",
    "        metrics_list.append(fold_metrics)\n",
    "        \n",
    "        # # ðŸ“¤ Print metrics immediately\n",
    "        # print(f\"Fold {len(metrics_list)} results:\")\n",
    "        # for metric, value in fold_metrics.items():\n",
    "        #     print(f\"  {metric}: {value:.4f}\" if value is not None else f\"  {metric}: None\")\n",
    "\n",
    "    # Average metrics over folds\n",
    "    avg_metrics = {}\n",
    "    # We know the keys from one fold's result\n",
    "    keys = metrics_list[0].keys()\n",
    "    for k in keys:\n",
    "        # Some might be None if there's an issue with ROC-AUC\n",
    "        vals = [m[k] for m in metrics_list if m[k] is not None]\n",
    "        if len(vals) > 0:\n",
    "            avg_metrics[k] = np.mean(vals)\n",
    "        else:\n",
    "            avg_metrics[k] = None\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "def multiple_seeds_5nn(distance_matrix, labels, seeds, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Run the 5-fold 5-NN classification for multiple random_state seeds,\n",
    "    then average the metrics across all runs.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): NxN precomputed distances.\n",
    "        labels (array-like): length N array of class labels.\n",
    "        seeds (list of ints): A list of random seeds to test.\n",
    "        n_neighbors (int): k in k-NN (default=5).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of overall average metrics across all seeds.\n",
    "    \"\"\"\n",
    "    all_runs_metrics = []\n",
    "    for seed in seeds:\n",
    "        run_metrics = five_fold_5nn(distance_matrix, labels,\n",
    "                                    n_neighbors=n_neighbors,\n",
    "                                    random_state=seed)\n",
    "        all_runs_metrics.append(run_metrics)\n",
    "\n",
    "    # Average across seeds\n",
    "    avg_overall = {}\n",
    "    # we'll look at keys from the first run\n",
    "    keys = all_runs_metrics[0].keys()\n",
    "    for k in keys:\n",
    "        vals = [run[k] for run in all_runs_metrics if run[k] is not None]\n",
    "        if len(vals) > 0:\n",
    "            avg_overall[k] = np.mean(vals)\n",
    "        else:\n",
    "            avg_overall[k] = None\n",
    "\n",
    "    return avg_overall\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Configuration\n",
    "# -------------------------------\n",
    "\n",
    "features_needed = [('facet', 0)]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. Main\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # (Optional) Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Example: Suppose you have data in a CSV and features in a .npy file\n",
    "    # similar to your Siamese approach\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    for encoder in encoders:\n",
    "        print(\"encoder \", encoder)\n",
    "        for data_name in csv_names:\n",
    "            print(\"data \", data_name)\n",
    "            for k in range(4,5):\n",
    "                print(\"k \", k)\n",
    "\n",
    "                for feature, max_dim in features_needed:\n",
    "                    print(feature)\n",
    "                    print(max_dim)\n",
    "                    path_to_data = f'data/{data_name}'\n",
    "                    df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "                    x_list = df['Accession (version)'].to_list()\n",
    "                    y_list = df['Family'].to_list()\n",
    "                    x_list, y_list = zip(*sorted(zip(x_list, y_list)))\n",
    "                    x_list = list(x_list)\n",
    "                    y_list = list(y_list)\n",
    "                    indices = find_labels_with_min_count(y_list, min_count=15)\n",
    "                    x_list = [x_list[i] for i in indices]\n",
    "                    y_list = [y_list[i] for i in indices]\n",
    "\n",
    "\n",
    "                    path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # for d in range(max_dim + 1):\n",
    "                    # print(d)\n",
    "                    distance_matrix = np.load(path_to_features+f'/k{k}_distance_{feature}{max_dim}.npy')\n",
    "                    subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    seeds = np.arange(1,31)\n",
    "                    le = LabelEncoder()\n",
    "                    y_int = le.fit_transform(y_list)  # shape (N,)\n",
    "                    y_np = np.array(y_int, dtype=np.int32)\n",
    "                    labels = y_np\n",
    "                    results = multiple_seeds_5nn(subD, labels, seeds, n_neighbors=5)\n",
    "                    print(\"Overall average metrics across seeds:\")\n",
    "                    for metric_name, val in results.items():\n",
    "                        print(f\"{metric_name}: {val}\")\n",
    "\n",
    "\n",
    "\n",
    "                    print(\"=================================================================================\")\n",
    "                    print(\"=================================================================================\")\n",
    "                    print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder  PSRT\n",
      "data  NCBI_record_valid_nucleotide\n",
      "k  4\n",
      "facet\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall average metrics across seeds:\n",
      "accuracy: 0.8759404096834262\n",
      "balanced_accuracy: 0.7916106489739025\n",
      "precision_macro: 0.8529276718534098\n",
      "recall_macro: 0.7916106489739025\n",
      "f1_macro: 0.8044019815704782\n",
      "roc_auc_macro: 0.8952229908522413\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "# testing the folds\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    'PSRT',\n",
    "]\n",
    "\n",
    "csv_names = [\n",
    "    'NCBI_record_valid_nucleotide',\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 0. Seed Setting (Optional)\n",
    "###############################################################################\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def find_labels_with_min_count(labels, min_count=15):\n",
    "    \"\"\"\n",
    "    Find the positions of labels whose count is greater than or equal to `min_count`.\n",
    "\n",
    "    Args:\n",
    "        labels (list): A list of labels.\n",
    "        min_count (int): The minimum count to filter labels (default=15).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of labels that meet the count condition.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Find labels with count >= min_count\n",
    "    valid_labels = {label for label, count in label_counts.items() if count >= min_count}\n",
    "    \n",
    "    # Find indices of valid labels\n",
    "    indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_classification_metrics(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Compute multiple classification metrics given true and predicted labels.\n",
    "    Returns a dictionary with:\n",
    "        - accuracy\n",
    "        - balanced_accuracy\n",
    "        - precision\n",
    "        - recall\n",
    "        - f1\n",
    "        - roc_auc (if multi-class or binary)\n",
    "\n",
    "    Note: For ROC-AUC with discrete predictions, we do a one-hot approach with\n",
    "    'ovr' to handle multi-class. If there's only one class in y_true, ROC-AUC is None.\n",
    "    \"\"\"\n",
    "    metrics_dict = {}\n",
    "\n",
    "    # Basic stats\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    metrics_dict['accuracy'] = acc\n",
    "    metrics_dict['balanced_accuracy'] = bal_acc\n",
    "    metrics_dict[f'precision_{average}'] = prec\n",
    "    metrics_dict[f'recall_{average}'] = rec\n",
    "    metrics_dict[f'f1_{average}'] = f1\n",
    "\n",
    "    # Attempt a multi-class or binary ROC-AUC with discrete predictions\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    try:\n",
    "        y_true_1hot = np.zeros((len(y_true), num_classes), dtype=int)\n",
    "        y_pred_1hot = np.zeros((len(y_pred), num_classes), dtype=int)\n",
    "        for i, lbl in enumerate(y_true):\n",
    "            y_true_1hot[i, lbl] = 1\n",
    "        for i, lbl in enumerate(y_pred):\n",
    "            y_pred_1hot[i, lbl] = 1\n",
    "\n",
    "        roc_val = roc_auc_score(\n",
    "            y_true_1hot,\n",
    "            y_pred_1hot,\n",
    "            multi_class='ovr',\n",
    "            average=average\n",
    "        )\n",
    "        metrics_dict[f'roc_auc_{average}'] = roc_val\n",
    "    except ValueError:\n",
    "        # e.g., if y_true has only one class\n",
    "        metrics_dict[f'roc_auc_{average}'] = None\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def five_fold_5nn(distance_matrix, labels, n_neighbors=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform 5-fold stratified cross-validation using a distance matrix and\n",
    "    5-NN classification. Computes multiple metrics on each fold and returns\n",
    "    their average.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): NxN precomputed distances.\n",
    "        labels (array-like): length N array of class labels.\n",
    "        n_neighbors (int): k in k-NN (default=5).\n",
    "        random_state (int): seed for StratifiedKFold reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of average metrics (accuracy, balanced_accuracy,\n",
    "              precision_weighted, recall_weighted, f1_weighted, roc_auc_weighted).\n",
    "    \"\"\"\n",
    "    distance_matrix = np.asarray(distance_matrix)\n",
    "    labels = np.asarray(labels)\n",
    "    N = distance_matrix.shape[0]\n",
    "    assert distance_matrix.shape == (N, N), \"distance_matrix must be NxN\"\n",
    "    assert len(labels) == N, \"labels length must match distance_matrix size\"\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # We'll accumulate metrics over the 5 folds\n",
    "    metrics_list = []\n",
    "    for train_idx, test_idx in skf.split(X=labels, y=labels):\n",
    "        fold_preds = []\n",
    "        fold_true = []\n",
    "\n",
    "        for test_sample in test_idx:\n",
    "            # Distances from test_sample to all others\n",
    "            dist_row = distance_matrix[test_sample].copy()\n",
    "            # Exclude the sample itself\n",
    "            dist_row[test_sample] = np.inf\n",
    "\n",
    "            # Only consider training set distances\n",
    "            train_distances = dist_row[train_idx]\n",
    "\n",
    "            # Indices in 'train_idx' of the n_neighbors nearest neighbors\n",
    "            nn_in_train = np.argpartition(train_distances, n_neighbors)[:n_neighbors]\n",
    "            nn_global_idx = train_idx[nn_in_train]\n",
    "\n",
    "            # Majority vote\n",
    "            neighbor_labels = labels[nn_global_idx]\n",
    "            chosen_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "\n",
    "            fold_preds.append(chosen_label)\n",
    "            fold_true.append(labels[test_sample])\n",
    "\n",
    "        # Compute metrics for this fold\n",
    "        fold_metrics = compute_classification_metrics(fold_true, fold_preds, average='macro')\n",
    "        metrics_list.append(fold_metrics)\n",
    "        \n",
    "        # # ðŸ“¤ Print metrics immediately\n",
    "        # print(f\"Fold {len(metrics_list)} results:\")\n",
    "        # for metric, value in fold_metrics.items():\n",
    "        #     print(f\"  {metric}: {value:.4f}\" if value is not None else f\"  {metric}: None\")\n",
    "\n",
    "    # Average metrics over folds\n",
    "    avg_metrics = {}\n",
    "    # We know the keys from one fold's result\n",
    "    keys = metrics_list[0].keys()\n",
    "    for k in keys:\n",
    "        # Some might be None if there's an issue with ROC-AUC\n",
    "        vals = [m[k] for m in metrics_list if m[k] is not None]\n",
    "        if len(vals) > 0:\n",
    "            avg_metrics[k] = np.mean(vals)\n",
    "        else:\n",
    "            avg_metrics[k] = None\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "def multiple_seeds_5nn(distance_matrix, labels, seeds, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Run the 5-fold 5-NN classification for multiple random_state seeds,\n",
    "    then average the metrics across all runs.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): NxN precomputed distances.\n",
    "        labels (array-like): length N array of class labels.\n",
    "        seeds (list of ints): A list of random seeds to test.\n",
    "        n_neighbors (int): k in k-NN (default=5).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of overall average metrics across all seeds.\n",
    "    \"\"\"\n",
    "    all_runs_metrics = []\n",
    "    for seed in seeds:\n",
    "        run_metrics = five_fold_5nn(distance_matrix, labels,\n",
    "                                    n_neighbors=n_neighbors,\n",
    "                                    random_state=seed)\n",
    "        all_runs_metrics.append(run_metrics)\n",
    "\n",
    "    # Average across seeds\n",
    "    avg_overall = {}\n",
    "    # we'll look at keys from the first run\n",
    "    keys = all_runs_metrics[0].keys()\n",
    "    for k in keys:\n",
    "        vals = [run[k] for run in all_runs_metrics if run[k] is not None]\n",
    "        if len(vals) > 0:\n",
    "            avg_overall[k] = np.mean(vals)\n",
    "        else:\n",
    "            avg_overall[k] = None\n",
    "\n",
    "    return avg_overall\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Configuration\n",
    "# -------------------------------\n",
    "\n",
    "features_needed = [('facet', 0)]\n",
    "\n",
    "###############################################################################\n",
    "# 5. Main\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # (Optional) Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Example: Suppose you have data in a CSV and features in a .npy file\n",
    "    # similar to your Siamese approach\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    for encoder in encoders:\n",
    "        print(\"encoder \", encoder)\n",
    "        for data_name in csv_names:\n",
    "            print(\"data \", data_name)\n",
    "            for k in range(4,5):\n",
    "                print(\"k \", k)\n",
    "\n",
    "                for feature, max_dim in features_needed:\n",
    "                    print(feature)\n",
    "                    print(max_dim)\n",
    "                    path_to_data = f'data/{data_name}'\n",
    "                    df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "                    x_list = df['Accession (version)'].to_list()\n",
    "                    y_list = df['Family'].to_list()\n",
    "                    x_list, y_list = zip(*sorted(zip(x_list, y_list)))\n",
    "                    x_list = list(x_list)\n",
    "                    y_list = list(y_list)\n",
    "                    indices = find_labels_with_min_count(y_list, min_count=15)\n",
    "                    x_list = [x_list[i] for i in indices]\n",
    "                    y_list = [y_list[i] for i in indices]\n",
    "\n",
    "\n",
    "                    path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # for d in range(max_dim + 1):\n",
    "                    # print(d)\n",
    "                    distance_matrix = np.load(path_to_features+f'/k{k}_distance_{feature}{max_dim}.npy')\n",
    "                    subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    seeds = np.arange(1,31)\n",
    "                    le = LabelEncoder()\n",
    "                    y_int = le.fit_transform(y_list)  # shape (N,)\n",
    "                    y_np = np.array(y_int, dtype=np.int32)\n",
    "                    labels = y_np\n",
    "                    results = multiple_seeds_5nn(subD, labels, seeds, n_neighbors=5)\n",
    "                    print(\"Overall average metrics across seeds:\")\n",
    "                    for metric_name, val in results.items():\n",
    "                        print(f\"{metric_name}: {val}\")\n",
    "\n",
    "\n",
    "\n",
    "                    print(\"=================================================================================\")\n",
    "                    print(\"=================================================================================\")\n",
    "                    print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI 2024 All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder  PSRT\n",
      "data  NCBI_record_valid_count\n",
      "k  4\n",
      "facet\n",
      "0\n",
      "Overall average metrics across seeds:\n",
      "accuracy: 0.8759067882472139\n",
      "balanced_accuracy: 0.79430174045709\n",
      "precision_macro: 0.8531756964988015\n",
      "recall_macro: 0.79430174045709\n",
      "f1_macro: 0.8066358920785363\n",
      "roc_auc_macro: 0.8966229384573784\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "# testing the folds\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    'PSRT',\n",
    "]\n",
    "\n",
    "csv_names = [\n",
    "    'NCBI_record_valid_count',\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 0. Seed Setting (Optional)\n",
    "###############################################################################\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def find_labels_with_min_count(labels, min_count=15):\n",
    "    \"\"\"\n",
    "    Find the positions of labels whose count is greater than or equal to `min_count`.\n",
    "\n",
    "    Args:\n",
    "        labels (list): A list of labels.\n",
    "        min_count (int): The minimum count to filter labels (default=15).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of labels that meet the count condition.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Find labels with count >= min_count\n",
    "    valid_labels = {label for label, count in label_counts.items() if count >= min_count}\n",
    "    \n",
    "    # Find indices of valid labels\n",
    "    indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_classification_metrics(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Compute multiple classification metrics given true and predicted labels.\n",
    "    Returns a dictionary with:\n",
    "        - accuracy\n",
    "        - balanced_accuracy\n",
    "        - precision\n",
    "        - recall\n",
    "        - f1\n",
    "        - roc_auc (if multi-class or binary)\n",
    "\n",
    "    Note: For ROC-AUC with discrete predictions, we do a one-hot approach with\n",
    "    'ovr' to handle multi-class. If there's only one class in y_true, ROC-AUC is None.\n",
    "    \"\"\"\n",
    "    metrics_dict = {}\n",
    "\n",
    "    # Basic stats\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    metrics_dict['accuracy'] = acc\n",
    "    metrics_dict['balanced_accuracy'] = bal_acc\n",
    "    metrics_dict[f'precision_{average}'] = prec\n",
    "    metrics_dict[f'recall_{average}'] = rec\n",
    "    metrics_dict[f'f1_{average}'] = f1\n",
    "\n",
    "    # Attempt a multi-class or binary ROC-AUC with discrete predictions\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    try:\n",
    "        y_true_1hot = np.zeros((len(y_true), num_classes), dtype=int)\n",
    "        y_pred_1hot = np.zeros((len(y_pred), num_classes), dtype=int)\n",
    "        for i, lbl in enumerate(y_true):\n",
    "            y_true_1hot[i, lbl] = 1\n",
    "        for i, lbl in enumerate(y_pred):\n",
    "            y_pred_1hot[i, lbl] = 1\n",
    "\n",
    "        roc_val = roc_auc_score(\n",
    "            y_true_1hot,\n",
    "            y_pred_1hot,\n",
    "            multi_class='ovr',\n",
    "            average=average\n",
    "        )\n",
    "        metrics_dict[f'roc_auc_{average}'] = roc_val\n",
    "    except ValueError:\n",
    "        # e.g., if y_true has only one class\n",
    "        metrics_dict[f'roc_auc_{average}'] = None\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def five_fold_5nn(distance_matrix, labels, n_neighbors=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform 5-fold stratified cross-validation using a distance matrix and\n",
    "    5-NN classification. Computes multiple metrics on each fold and returns\n",
    "    their average.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): NxN precomputed distances.\n",
    "        labels (array-like): length N array of class labels.\n",
    "        n_neighbors (int): k in k-NN (default=5).\n",
    "        random_state (int): seed for StratifiedKFold reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of average metrics (accuracy, balanced_accuracy,\n",
    "              precision_weighted, recall_weighted, f1_weighted, roc_auc_weighted).\n",
    "    \"\"\"\n",
    "    distance_matrix = np.asarray(distance_matrix)\n",
    "    labels = np.asarray(labels)\n",
    "    N = distance_matrix.shape[0]\n",
    "    assert distance_matrix.shape == (N, N), \"distance_matrix must be NxN\"\n",
    "    assert len(labels) == N, \"labels length must match distance_matrix size\"\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # We'll accumulate metrics over the 5 folds\n",
    "    metrics_list = []\n",
    "    for train_idx, test_idx in skf.split(X=labels, y=labels):\n",
    "        fold_preds = []\n",
    "        fold_true = []\n",
    "\n",
    "        for test_sample in test_idx:\n",
    "            # Distances from test_sample to all others\n",
    "            dist_row = distance_matrix[test_sample].copy()\n",
    "            # Exclude the sample itself\n",
    "            dist_row[test_sample] = np.inf\n",
    "\n",
    "            # Only consider training set distances\n",
    "            train_distances = dist_row[train_idx]\n",
    "\n",
    "            # Indices in 'train_idx' of the n_neighbors nearest neighbors\n",
    "            nn_in_train = np.argpartition(train_distances, n_neighbors)[:n_neighbors]\n",
    "            nn_global_idx = train_idx[nn_in_train]\n",
    "\n",
    "            # Majority vote\n",
    "            neighbor_labels = labels[nn_global_idx]\n",
    "            chosen_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "\n",
    "            fold_preds.append(chosen_label)\n",
    "            fold_true.append(labels[test_sample])\n",
    "\n",
    "        # Compute metrics for this fold\n",
    "        fold_metrics = compute_classification_metrics(fold_true, fold_preds, average='macro')\n",
    "        metrics_list.append(fold_metrics)\n",
    "        \n",
    "        # # ðŸ“¤ Print metrics immediately\n",
    "        # print(f\"Fold {len(metrics_list)} results:\")\n",
    "        # for metric, value in fold_metrics.items():\n",
    "        #     print(f\"  {metric}: {value:.4f}\" if value is not None else f\"  {metric}: None\")\n",
    "\n",
    "    # Average metrics over folds\n",
    "    avg_metrics = {}\n",
    "    # We know the keys from one fold's result\n",
    "    keys = metrics_list[0].keys()\n",
    "    for k in keys:\n",
    "        # Some might be None if there's an issue with ROC-AUC\n",
    "        vals = [m[k] for m in metrics_list if m[k] is not None]\n",
    "        if len(vals) > 0:\n",
    "            avg_metrics[k] = np.mean(vals)\n",
    "        else:\n",
    "            avg_metrics[k] = None\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "def multiple_seeds_5nn(distance_matrix, labels, seeds, n_neighbors=5):\n",
    "    \"\"\"\n",
    "    Run the 5-fold 5-NN classification for multiple random_state seeds,\n",
    "    then average the metrics across all runs.\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): NxN precomputed distances.\n",
    "        labels (array-like): length N array of class labels.\n",
    "        seeds (list of ints): A list of random seeds to test.\n",
    "        n_neighbors (int): k in k-NN (default=5).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of overall average metrics across all seeds.\n",
    "    \"\"\"\n",
    "    all_runs_metrics = []\n",
    "    for seed in seeds:\n",
    "        run_metrics = five_fold_5nn(distance_matrix, labels,\n",
    "                                    n_neighbors=n_neighbors,\n",
    "                                    random_state=seed)\n",
    "        all_runs_metrics.append(run_metrics)\n",
    "\n",
    "    # Average across seeds\n",
    "    avg_overall = {}\n",
    "    # we'll look at keys from the first run\n",
    "    keys = all_runs_metrics[0].keys()\n",
    "    for k in keys:\n",
    "        vals = [run[k] for run in all_runs_metrics if run[k] is not None]\n",
    "        if len(vals) > 0:\n",
    "            avg_overall[k] = np.mean(vals)\n",
    "        else:\n",
    "            avg_overall[k] = None\n",
    "\n",
    "    return avg_overall\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Configuration\n",
    "# -------------------------------\n",
    "\n",
    "features_needed = [('facet', 0)]\n",
    "\n",
    "###############################################################################\n",
    "# 5. Main\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # (Optional) Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Example: Suppose you have data in a CSV and features in a .npy file\n",
    "    # similar to your Siamese approach\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    for encoder in encoders:\n",
    "        print(\"encoder \", encoder)\n",
    "        for data_name in csv_names:\n",
    "            print(\"data \", data_name)\n",
    "            for k in range(4,5):\n",
    "                print(\"k \", k)\n",
    "\n",
    "                for feature, max_dim in features_needed:\n",
    "                    print(feature)\n",
    "                    print(max_dim)\n",
    "                    path_to_data = f'data/{data_name}'\n",
    "                    df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "                    x_list = df['Accession (version)'].to_list()\n",
    "                    y_list = df['Family'].to_list()\n",
    "                    x_list, y_list = zip(*sorted(zip(x_list, y_list)))\n",
    "                    x_list = list(x_list)\n",
    "                    y_list = list(y_list)\n",
    "                    indices = find_labels_with_min_count(y_list, min_count=15)\n",
    "                    x_list = [x_list[i] for i in indices]\n",
    "                    y_list = [y_list[i] for i in indices]\n",
    "\n",
    "\n",
    "                    path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # for d in range(max_dim + 1):\n",
    "                    # print(d)\n",
    "                    distance_matrix = np.load(path_to_features+f'/k{k}_distance_{feature}{max_dim}.npy')\n",
    "                    subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    seeds = np.arange(1,31)\n",
    "                    le = LabelEncoder()\n",
    "                    y_int = le.fit_transform(y_list)  # shape (N,)\n",
    "                    y_np = np.array(y_int, dtype=np.int32)\n",
    "                    labels = y_np\n",
    "                    results = multiple_seeds_5nn(subD, labels, seeds, n_neighbors=5)\n",
    "                    print(\"Overall average metrics across seeds:\")\n",
    "                    for metric_name, val in results.items():\n",
    "                        print(f\"{metric_name}: {val}\")\n",
    "\n",
    "\n",
    "\n",
    "                    print(\"=================================================================================\")\n",
    "                    print(\"=================================================================================\")\n",
    "                    print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "                print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n",
    "            print(\"=================================================================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
