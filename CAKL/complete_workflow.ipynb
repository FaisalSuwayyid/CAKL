{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yau2020_record_processed\n",
      "4\n",
      "sample  0\n",
      "accession  NC_010314.1\n",
      "sample  1\n",
      "accession  NC_010315.1\n",
      "sample  2\n",
      "accession  NC_010316.1\n",
      "sample  3\n",
      "accession  NC_010317.1\n",
      "sample  4\n",
      "accession  NC_010318.1\n",
      "sample  5\n",
      "accession  NC_010319.1\n",
      "sample  6\n",
      "accession  NC_018874.1\n",
      "sample  7\n",
      "accession  NC_001499.1\n",
      "sample  8\n",
      "accession  NC_014138.1\n",
      "sample  9\n",
      "accession  NC_014139.1\n",
      "sample  10\n",
      "accession  NC_015045.1\n",
      "sample  11\n",
      "accession  NC_015048.1\n",
      "sample  12\n",
      "accession  NC_016574.1\n",
      "sample  13\n",
      "accession  NC_016577.1\n",
      "sample  14\n",
      "accession  NC_001928.2\n",
      "sample  15\n",
      "accession  NC_001929.2\n",
      "sample  16\n",
      "accession  NC_014649.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 156\u001b[0m\n\u001b[1;32m    154\u001b[0m f_curves \u001b[38;5;241m=\u001b[39m ph\u001b[38;5;241m.\u001b[39mcompute_f_vector_curves()\n\u001b[1;32m    155\u001b[0m h_curves \u001b[38;5;241m=\u001b[39m ph\u001b[38;5;241m.\u001b[39mcompute_h_vector_curves()\n\u001b[0;32m--> 156\u001b[0m facet_curves \u001b[38;5;241m=\u001b[39m ph\u001b[38;5;241m.\u001b[39mfacet_curves()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Stack curves per dimension\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_dimension \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/code/cakl_github/CAKL-main 6/CAKL/psrt.py:345\u001b[0m, in \u001b[0;36mPH.facet_curves\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecific_filtration \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_edge_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_edge_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_instances)\n\u001b[1;32m    344\u001b[0m t_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecific_filtration\n\u001b[0;32m--> 345\u001b[0m curves \u001b[38;5;241m=\u001b[39m {dimension: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_active_curve(dimension, t_values) \u001b[38;5;28;01mfor\u001b[39;00m dimension \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_dimension \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)}\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m curves\n",
      "File \u001b[0;32m~/Desktop/code/cakl_github/CAKL-main 6/CAKL/psrt.py:331\u001b[0m, in \u001b[0;36mPH.compute_active_curve\u001b[0;34m(self, dimension, t_values)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_active_curve\u001b[39m(\u001b[38;5;28mself\u001b[39m, dimension, t_values):\n\u001b[0;32m--> 331\u001b[0m     births, deaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_intervals_from_barcodes(dimension\u001b[38;5;241m=\u001b[39mdimension)\n\u001b[1;32m    332\u001b[0m     counts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount_active_intervals_sorted(births, deaths, t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m t_values]\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m counts\n",
      "File \u001b[0;32m~/Desktop/code/cakl_github/CAKL-main 6/CAKL/psrt.py:314\u001b[0m, in \u001b[0;36mPH.prepare_intervals_from_barcodes\u001b[0;34m(self, dimension)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_intervals_from_barcodes\u001b[39m(\u001b[38;5;28mself\u001b[39m, dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 314\u001b[0m     barcodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvietoris_rips_simplices_birth_death()\n\u001b[1;32m    315\u001b[0m     intervals \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dimension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/code/cakl_github/CAKL-main 6/CAKL/psrt.py:299\u001b[0m, in \u001b[0;36mPH.vietoris_rips_simplices_birth_death\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m subset:\n\u001b[0;32m--> 299\u001b[0m         superset \u001b[38;5;241m=\u001b[39m subset\u001b[38;5;241m.\u001b[39munion({v})\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(superset) \u001b[38;5;241m==\u001b[39m k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m superset \u001b[38;5;129;01min\u001b[39;00m subset_birth:\n\u001b[1;32m    301\u001b[0m             candidate_deaths\u001b[38;5;241m.\u001b[39mappend(subset_birth[superset])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Import necessary libraries\n",
    "# -------------------------------\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Local module\n",
    "from psrt import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Featurization\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoder = 'PSRT'\n",
    "max_dimension = 2\n",
    "num = 2\n",
    "k_chosen = 4\n",
    "\n",
    "csv_names = [\n",
    "    'Yau2020_record_processed',\n",
    "    # 'Yau2022_record_processed',\n",
    "    # 'NCBI_record_valid_nucleotide',\n",
    "    # 'NCBI_record_valid_count',\n",
    "]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration parameters\n",
    "# -------------------------------\n",
    "# encoder = 'PSRT'\n",
    "# max_dimension = 2\n",
    "alphabet = [\"A\", \"C\", \"G\", \"T\"]\n",
    "# num = 0\n",
    "\n",
    "# List of dataset base names (CSV and FASTA share the same prefix)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Loop over datasets\n",
    "# -------------------------------\n",
    "for data_name in csv_names:\n",
    "    print(data_name)\n",
    "    path_to_data = f\"./data/{data_name}\"\n",
    "\n",
    "    # Loop over k-mer sizes from 1 to 7\n",
    "    for k in range(k_chosen, k_chosen+1):\n",
    "        print(k)\n",
    "\n",
    "        # Load accessions from CSV\n",
    "        df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "        x = df['Accession (version)'].to_list()\n",
    "\n",
    "        # Parse FASTA sequences into a list of SeqRecord objects\n",
    "        file_path = f'{path_to_data}.fasta'\n",
    "        DNAs = list(SeqIO.parse(file_path, \"fasta\"))\n",
    "\n",
    "        # Create a dictionary: accession → SeqRecord\n",
    "        dna_dict = {record.id: record for record in DNAs}\n",
    "\n",
    "        # Match sequences with accessions from CSV, preserving order\n",
    "        ordered_dnas = [dna_dict[accession] for accession in x if accession in dna_dict]\n",
    "        dna_list = [str(record.seq) for record in ordered_dnas]\n",
    "\n",
    "        # Generate all possible k-mers of length k\n",
    "        kmers = generate_all_kmers(alphabet, k)\n",
    "\n",
    "        # Define filtration values (uniform grid scaled by 4^k)\n",
    "        specific_filtration = np.array([i * 4**k for i in range(0, num + 1)])\n",
    "\n",
    "        # -------------------------------\n",
    "        # Process each DNA sequence\n",
    "        # -------------------------------\n",
    "        # for i in range(len(dna_list)):\n",
    "        for i in range(len(dna_list)):\n",
    "            print(\"sample \", i)\n",
    "            print(\"accession \", ordered_dnas[i].id)\n",
    "            dna = dna_list[i]  # ← fix: define current sequence\n",
    "\n",
    "            # Initialize per-dimension feature accumulators\n",
    "            betti_result = (max_dimension + 1) * [None]\n",
    "            f_result = (max_dimension + 1) * [None]\n",
    "            h_result = (max_dimension + 2) * [None]\n",
    "            facet_result = (max_dimension + 1) * [None]\n",
    "\n",
    "            # -------------------------------\n",
    "            # Loop over all possible k-mers\n",
    "            # -------------------------------\n",
    "            for kmer in kmers:\n",
    "                points = occurrence(dna, kmer)\n",
    "\n",
    "                # Handle case where k-mer is absent\n",
    "                if len(points) == 0:\n",
    "                    zero_curve = np.array((1 + num) * [0])\n",
    "                    betti_curves = (max_dimension + 1) * [zero_curve]\n",
    "                    f_curves = (max_dimension + 1) * [zero_curve]\n",
    "                    h_curves = (max_dimension + 2) * [zero_curve]\n",
    "                    facet_curves = (max_dimension + 1) * [zero_curve]\n",
    "\n",
    "                    # Append zero curves to result arrays\n",
    "                    for d in range(max_dimension + 1):\n",
    "                        if betti_result[d] is None:\n",
    "                            betti_result[d] = betti_curves[d]\n",
    "                        else:\n",
    "                            betti_result[d] = np.vstack([betti_result[d], betti_curves[d]])\n",
    "\n",
    "                        if f_result[d] is None:\n",
    "                            f_result[d] = f_curves[d]\n",
    "                        else:\n",
    "                            f_result[d] = np.vstack([f_result[d], f_curves[d]])\n",
    "\n",
    "                        if facet_result[d] is None:\n",
    "                            facet_result[d] = facet_curves[d]\n",
    "                        else:\n",
    "                            facet_result[d] = np.vstack([facet_result[d], facet_curves[d]])\n",
    "\n",
    "                    # Append zero curves to result arrays\n",
    "                    for d in range(max_dimension + 2):\n",
    "                        if h_result[d] is None:\n",
    "                            h_result[d] = h_curves[d]\n",
    "                        else:\n",
    "                            h_result[d] = np.vstack([h_result[d], h_curves[d]])\n",
    "                    continue\n",
    "\n",
    "                # -------------------------------\n",
    "                # Compute PH features for current k-mer\n",
    "                # -------------------------------\n",
    "                points = np.array(points)[:, np.newaxis]  # Reshape to 2D\n",
    "\n",
    "                ph = PH(\n",
    "                    points,\n",
    "                    max_dimension=max_dimension,\n",
    "                    max_edge_length=2.0,\n",
    "                    specific_filtration=specific_filtration\n",
    "                )\n",
    "\n",
    "                alphas, betti_curves = ph.betti_curves()\n",
    "                f_curves = ph.compute_f_vector_curves()\n",
    "                h_curves = ph.compute_h_vector_curves()\n",
    "                facet_curves = ph.facet_curves()\n",
    "\n",
    "                # Stack curves per dimension\n",
    "                for d in range(max_dimension + 1):\n",
    "                    if betti_result[d] is None:\n",
    "                        betti_result[d] = betti_curves.get(d, np.zeros_like(alphas))\n",
    "                    else:\n",
    "                        betti_result[d] = np.vstack([betti_result[d], betti_curves.get(d, np.zeros_like(alphas))])\n",
    "\n",
    "                    if f_result[d] is None:\n",
    "                        f_result[d] = f_curves.get(d, np.zeros_like(alphas))\n",
    "                    else:\n",
    "                        f_result[d] = np.vstack([f_result[d], f_curves.get(d, np.zeros_like(alphas))])\n",
    "\n",
    "                    if facet_result[d] is None:\n",
    "                        facet_result[d] = facet_curves.get(d, np.zeros_like(alphas))\n",
    "                    else:\n",
    "                        facet_result[d] = np.vstack([facet_result[d], facet_curves.get(d, np.zeros_like(alphas))])\n",
    "\n",
    "                # Stack curves per dimension\n",
    "                for d in range(max_dimension + 2):\n",
    "                    if h_result[d] is None:\n",
    "                        h_result[d] = h_curves.get(d, np.zeros_like(alphas))\n",
    "                    else:\n",
    "                        h_result[d] = np.vstack([h_result[d], h_curves.get(d, np.zeros_like(alphas))])\n",
    "\n",
    "            # -------------------------------\n",
    "            # Save results to .npy files\n",
    "            # -------------------------------\n",
    "            save_path = f\"features/{encoder}/{data_name}/{k}\"\n",
    "            os.makedirs(save_path, exist_ok=True)   # create folder path if missing\n",
    "            for d in range(max_dimension+1):\n",
    "                np.save(f\"./features/{encoder}/{data_name}/{k}/{x[i]}_betti{d}.npy\", betti_result[d])\n",
    "                np.save(f\"./features/{encoder}/{data_name}/{k}/{x[i]}_f{d}\", f_result[d])\n",
    "                np.save(f\"./features/{encoder}/{data_name}/{k}/{x[i]}_facet{d}\", facet_result[d])\n",
    "\n",
    "            # -------------------------------\n",
    "            # Save results to .npy files\n",
    "            # -------------------------------\n",
    "            for d in range(max_dimension+2):\n",
    "                np.save(f\"./features/{encoder}/{data_name}/{k}/{x[i]}_h{d}\", h_result[d])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Stacking\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "encoder = 'PSRT'\n",
    "k_chosen = 4\n",
    "\n",
    "\n",
    "def stack_arrays_vertically(file_paths):\n",
    "    \"\"\"\n",
    "    Load NumPy arrays from given file paths and stack them vertically.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list of str): List of file paths to the NumPy arrays.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A single vertically stacked NumPy array.\n",
    "    \"\"\"\n",
    "    # Load each array and store it in a list\n",
    "    # arrays = [np.load(path) for path in file_paths]\n",
    "    arrays = [np.load(path).reshape(-1) for path in file_paths]\n",
    "    \n",
    "    # Stack the arrays vertically\n",
    "    stacked_array = np.vstack(arrays)\n",
    "    \n",
    "    return stacked_array\n",
    "\n",
    "\n",
    "# encoder = 'PSRT'\n",
    "\n",
    "features_needed = [('betti', 2), ('f', 2), ('facet', 2), ('h', 3)]\n",
    "\n",
    "\n",
    "for data_name in csv_names:\n",
    "    print(data_name)\n",
    "    for k in range(k_chosen,k_chosen+1):\n",
    "        print(k)\n",
    "        path_to_features = f'features/{encoder}/{data_name}/{k}'\n",
    "        path_to_data = f'data/{data_name}'\n",
    "\n",
    "        # Read CSV file into DataFrame\n",
    "        df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "        # Load and sort accession IDs from metadata\n",
    "        accessions = sorted(df['Accession (version)'].to_list())\n",
    "\n",
    "        # Path to folder with feature files\n",
    "        folder_path = path_to_features\n",
    "\n",
    "        for considered_feature, considered_max_dimension in features_needed:\n",
    "            print(considered_feature, considered_max_dimension)\n",
    "            for d in range(considered_max_dimension+1):\n",
    "                print(\"d \", d)\n",
    "\n",
    "                # List all betti0 .npy files\n",
    "                file_names = [\n",
    "                    f for f in os.listdir(folder_path)\n",
    "                    if f.endswith('.npy') and f'{considered_feature}{d}' in f and os.path.isfile(os.path.join(folder_path, f))\n",
    "                ]\n",
    "\n",
    "                # Extract accession IDs from filenames (assumes format: accession_betti0.npy)\n",
    "                modified_names = [f.rsplit(f'_{considered_feature}{d}', 1)[0] for f in file_names]\n",
    "\n",
    "                # Filter accession IDs that have corresponding feature files\n",
    "                matched_accessions = [acc for acc in accessions if acc in modified_names]\n",
    "                print(\"accessions \", len(accessions))\n",
    "                print(\"matched\" , len(matched_accessions))\n",
    "\n",
    "                # Map accession → index in file_names\n",
    "                element_to_index = {name: idx for idx, name in enumerate(modified_names)}\n",
    "\n",
    "                # Build index list (skip any accessions not found in file_names)\n",
    "                indices = [element_to_index[acc] for acc in matched_accessions if acc in element_to_index]\n",
    "\n",
    "                # Sort file names to match accession order\n",
    "                file_names_sorted = [file_names[i] for i in indices]\n",
    "\n",
    "                # Full paths to sorted .npy files\n",
    "                file_paths = [os.path.join(folder_path, fname) for fname in file_names_sorted]\n",
    "\n",
    "                np.save(f'features/{encoder}/{data_name}/k{k}_{considered_feature}{d}.npy', stack_arrays_vertically(file_paths))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Distance\n",
    "# -------------------------------\n",
    "\n",
    "# -------------------------------\n",
    "# Command-line Arguments\n",
    "# -------------------------------\n",
    "encoder = 'PSRT'\n",
    "k_chosen = 4\n",
    "\n",
    "# -------------------------------\n",
    "# Feature Configuration\n",
    "# -------------------------------\n",
    "features_needed = [('betti', 2), ('f', 2), ('facet', 2), ('h', 3)]\n",
    "\n",
    "# -------------------------------\n",
    "# Distance Computation Function\n",
    "# -------------------------------\n",
    "def compute_pairwise_distances(X, metric='euclidean', **kwargs):\n",
    "    \"\"\"\n",
    "    Compute pairwise distances for the dataset X.\n",
    "    \"\"\"\n",
    "    condensed_dist = pdist(X, metric=metric, **kwargs)\n",
    "    return squareform(condensed_dist)\n",
    "\n",
    "# -------------------------------\n",
    "# Main Script\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(f\"Encoder: {encoder}\")\n",
    "\n",
    "    for data_name in csv_names:\n",
    "        print(f\"Dataset: {data_name}\")\n",
    "\n",
    "        for k in range(k_chosen, k_chosen + 1):\n",
    "            print(f\"k = {k}\")\n",
    "\n",
    "            for feature, max_dim in features_needed:\n",
    "                print(f\"  Feature: {feature}\")\n",
    "\n",
    "                for d in range(max_dim + 1):\n",
    "                    input_path = f'features/{encoder}/{data_name}/k{k}_{feature}{d}.npy'\n",
    "\n",
    "                    if not os.path.exists(input_path):\n",
    "                        print(f\"    [Skipped] File not found: {input_path}\")\n",
    "                        continue\n",
    "\n",
    "                    # Load and normalize data\n",
    "                    X = np.load(input_path)\n",
    "                    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "                    # Compute distance matrix\n",
    "                    metric = 'minkowski'\n",
    "                    p = 2\n",
    "                    print(f\"    Computing pairwise distances (metric: {metric})...\")\n",
    "                    dist_matrix = compute_pairwise_distances(X, metric=metric, p=p)\n",
    "                    dist_matrix /= np.max(dist_matrix)\n",
    "\n",
    "                    # Save output\n",
    "                    output_dir = f'distances/{encoder}/{data_name}'\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    output_path = f'{output_dir}/k{k}_distance_{feature}{d}.npy'\n",
    "                    np.save(output_path, dist_matrix)\n",
    "\n",
    "                    print(f\"    [Saved] {output_path}\")\n",
    "\n",
    "                print(\"  \" + \"-\" * 50)\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
