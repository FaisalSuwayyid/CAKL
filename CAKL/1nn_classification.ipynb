{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder  PSRT\n",
      "data  Yau2020_record_processed\n",
      "k  4\n",
      "facet\n",
      "0\n",
      "Leave-One-Out k-NN Results (k=4)\n",
      "accuracy: 0.9319\n",
      "balanced_accuracy: 0.8664\n",
      "precision_macro: 0.8903\n",
      "recall_macro: 0.8664\n",
      "f1_macro: 0.8717\n",
      "roc_auc_macro: 0.9328\n",
      "===============================================\n",
      "===============================================\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "leave_one_out_knn_eval_distance.py\n",
    "\n",
    "Script to perform a leave-one-out 1-NN classification from a precomputed NxN\n",
    "distance matrix, and then compute various metrics:\n",
    "- Accuracy\n",
    "- Balanced Accuracy\n",
    "- Precision, Recall, F1\n",
    "- ROC AUC (multi-class or binary)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def leave_one_out_knn(distance_matrix, labels, n_neighbors=1):\n",
    "    \"\"\"\n",
    "    Perform leave-one-out k-NN (here k=1 by default) classification using\n",
    "    a precomputed NxN distance matrix.\n",
    "\n",
    "    For each sample i, we:\n",
    "      - Look at distance_matrix[i], which contains distances to all other samples.\n",
    "      - Exclude itself by setting distance_matrix[i,i] = np.inf (if not already).\n",
    "      - Find the nearest neighbor(s).\n",
    "      - Predict the label of the chosen neighbor(s).\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): shape (N, N), distance_matrix[i,j] is the\n",
    "                                   distance between samples i and j.\n",
    "        labels (ndarray): shape (N,), integer or string labels.\n",
    "        n_neighbors (int): number of neighbors (default=1).\n",
    "\n",
    "    Returns:\n",
    "        y_pred (ndarray): predicted labels for each sample (length N)\n",
    "    \"\"\"\n",
    "    N = distance_matrix.shape[0]\n",
    "    y_pred = np.zeros(N, dtype=labels.dtype)\n",
    "\n",
    "    for i in range(N):\n",
    "        # distances from sample i to all others\n",
    "        dist = distance_matrix[i].copy()\n",
    "        # exclude itself\n",
    "        dist[i] = np.inf\n",
    "\n",
    "        # find the index/indices of the nearest neighbor(s)\n",
    "        nn_idx = np.argpartition(dist, n_neighbors)[:n_neighbors]\n",
    "        if n_neighbors == 1:\n",
    "            chosen_label = labels[nn_idx[0]]\n",
    "        else:\n",
    "            # If k>1, we can do majority vote among chosen neighbors\n",
    "            chosen_labels = labels[nn_idx]\n",
    "            chosen_label = np.bincount(chosen_labels).argmax()\n",
    "\n",
    "        y_pred[i] = chosen_label\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, average='weighted'):\n",
    "    \"\"\"\n",
    "    Compute various classification metrics.\n",
    "    For multi-class, use 'average' = 'macro' or 'weighted'.\n",
    "\n",
    "    Returns a dict of metrics:\n",
    "    - accuracy\n",
    "    - balanced_accuracy\n",
    "    - precision_{average}\n",
    "    - recall_{average}\n",
    "    - f1_{average}\n",
    "    - roc_auc_{average}  (if applicable)\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        f'precision_{average}': prec,\n",
    "        f'recall_{average}': rec,\n",
    "        f'f1_{average}': f1\n",
    "    }\n",
    "\n",
    "    # For ROC AUC (multi-class or binary), we only have discrete predictions.\n",
    "    # We convert them to one-hot format for a 'one-vs-rest' approach in roc_auc_score.\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    try:\n",
    "        y_true_1hot = np.zeros((len(y_true), num_classes), dtype=int)\n",
    "        y_pred_1hot = np.zeros((len(y_pred), num_classes), dtype=int)\n",
    "\n",
    "        for i, lbl in enumerate(y_true):\n",
    "            y_true_1hot[i, lbl] = 1\n",
    "        for i, lbl in enumerate(y_pred):\n",
    "            y_pred_1hot[i, lbl] = 1\n",
    "\n",
    "        roc = roc_auc_score(\n",
    "            y_true_1hot,\n",
    "            y_pred_1hot,\n",
    "            multi_class='ovr',  # or 'ovo'\n",
    "            average=average\n",
    "        )\n",
    "        metrics_dict[f'roc_auc_{average}'] = roc\n",
    "\n",
    "    except ValueError:\n",
    "        # This can happen if there's only one class in y_true, etc.\n",
    "        metrics_dict[f'roc_auc_{average}'] = None\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def find_labels_with_min_count(labels, min_count=15):\n",
    "    \"\"\"\n",
    "    Find the positions of labels whose count is greater than or equal to `min_count`.\n",
    "\n",
    "    Args:\n",
    "        labels (list): A list of labels.\n",
    "        min_count (int): The minimum count to filter labels (default=15).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of labels that meet the count condition.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Find labels with count >= min_count\n",
    "    valid_labels = {label for label, count in label_counts.items() if count >= min_count}\n",
    "    \n",
    "    # Find indices of valid labels\n",
    "    indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    'PSRT',\n",
    "]\n",
    "\n",
    "csv_names = [\n",
    "    'Yau2020_record_processed',\n",
    "    # 'Yau2022_record_processed',\n",
    "    # 'NCBI_record_valid_nucleotide',\n",
    "    # 'NCBI_record_valid_count',\n",
    "]\n",
    "\n",
    "features_needed = [('facet', 0)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for encoder in encoders:\n",
    "        print(\"encoder \", encoder)\n",
    "        for data_name in csv_names:\n",
    "            print(\"data \", data_name)\n",
    "            for k in range(4,5):\n",
    "                print(\"k \", k)\n",
    "\n",
    "                for feature, max_dim in features_needed:\n",
    "                    print(feature)\n",
    "                    print(max_dim)\n",
    "                    path_to_data = f'data/{data_name}'\n",
    "                    df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "                    x_list = df['Accession (version)'].to_list()\n",
    "                    y_list = df['Family'].to_list()\n",
    "                    x_list, y_list = zip(*sorted(zip(x_list, y_list)))\n",
    "                    x_list = list(x_list)\n",
    "                    y_list = list(y_list)\n",
    "                    indices = find_labels_with_min_count(y_list, min_count=3)\n",
    "                    x_list = [x_list[i] for i in indices]\n",
    "                    y_list = [y_list[i] for i in indices]\n",
    "\n",
    "\n",
    "                    path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # for d in range(max_dim + 1):\n",
    "                    # print(d)\n",
    "                    distance_matrix = np.load(path_to_features+f'/k{k}_distance_{feature}{max_dim}.npy')\n",
    "                    subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\n",
    "                    # path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # # for d in range(max_dim + 1):\n",
    "                    # distance_matrix = np.load(path_to_features+f'/k{k}_distance.npy')\n",
    "                    # subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "                    le = LabelEncoder()\n",
    "                    y_int = le.fit_transform(y_list)  # shape (N,)\n",
    "                    y_np = np.array(y_int, dtype=np.int32)\n",
    "                    labels = y_np\n",
    "\n",
    "\n",
    "                    # 1-NN or k-NN using the distance matrix\n",
    "                    y_pred = leave_one_out_knn(subD, labels, n_neighbors=1)\n",
    "\n",
    "                    # Compute metrics\n",
    "                    metrics_result = compute_metrics(labels, y_pred, average='macro')\n",
    "\n",
    "                    # Print results\n",
    "                    print(\"Leave-One-Out k-NN Results (k={})\".format(k))\n",
    "                    for s, v in metrics_result.items():\n",
    "                        if v is not None:\n",
    "                            print(f\"{s}: {v:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"{s}: None\")\n",
    "                    print(\"===============================================\")\n",
    "                    print(\"===============================================\")\n",
    "                    print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder  PSRT\n",
      "data  Yau2022_record_processed\n",
      "k  4\n",
      "facet\n",
      "0\n",
      "Leave-One-Out k-NN Results (k=4)\n",
      "accuracy: 0.9198\n",
      "balanced_accuracy: 0.7668\n",
      "precision_macro: 0.8001\n",
      "recall_macro: 0.7668\n",
      "f1_macro: 0.7737\n",
      "roc_auc_macro: 0.8831\n",
      "===============================================\n",
      "===============================================\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "leave_one_out_knn_eval_distance.py\n",
    "\n",
    "Script to perform a leave-one-out 1-NN classification from a precomputed NxN\n",
    "distance matrix, and then compute various metrics:\n",
    "- Accuracy\n",
    "- Balanced Accuracy\n",
    "- Precision, Recall, F1\n",
    "- ROC AUC (multi-class or binary)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def leave_one_out_knn(distance_matrix, labels, n_neighbors=1):\n",
    "    \"\"\"\n",
    "    Perform leave-one-out k-NN (here k=1 by default) classification using\n",
    "    a precomputed NxN distance matrix.\n",
    "\n",
    "    For each sample i, we:\n",
    "      - Look at distance_matrix[i], which contains distances to all other samples.\n",
    "      - Exclude itself by setting distance_matrix[i,i] = np.inf (if not already).\n",
    "      - Find the nearest neighbor(s).\n",
    "      - Predict the label of the chosen neighbor(s).\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): shape (N, N), distance_matrix[i,j] is the\n",
    "                                   distance between samples i and j.\n",
    "        labels (ndarray): shape (N,), integer or string labels.\n",
    "        n_neighbors (int): number of neighbors (default=1).\n",
    "\n",
    "    Returns:\n",
    "        y_pred (ndarray): predicted labels for each sample (length N)\n",
    "    \"\"\"\n",
    "    N = distance_matrix.shape[0]\n",
    "    y_pred = np.zeros(N, dtype=labels.dtype)\n",
    "\n",
    "    for i in range(N):\n",
    "        # distances from sample i to all others\n",
    "        dist = distance_matrix[i].copy()\n",
    "        # exclude itself\n",
    "        dist[i] = np.inf\n",
    "\n",
    "        # find the index/indices of the nearest neighbor(s)\n",
    "        nn_idx = np.argpartition(dist, n_neighbors)[:n_neighbors]\n",
    "        if n_neighbors == 1:\n",
    "            chosen_label = labels[nn_idx[0]]\n",
    "        else:\n",
    "            # If k>1, we can do majority vote among chosen neighbors\n",
    "            chosen_labels = labels[nn_idx]\n",
    "            chosen_label = np.bincount(chosen_labels).argmax()\n",
    "\n",
    "        y_pred[i] = chosen_label\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, average='weighted'):\n",
    "    \"\"\"\n",
    "    Compute various classification metrics.\n",
    "    For multi-class, use 'average' = 'macro' or 'weighted'.\n",
    "\n",
    "    Returns a dict of metrics:\n",
    "    - accuracy\n",
    "    - balanced_accuracy\n",
    "    - precision_{average}\n",
    "    - recall_{average}\n",
    "    - f1_{average}\n",
    "    - roc_auc_{average}  (if applicable)\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        f'precision_{average}': prec,\n",
    "        f'recall_{average}': rec,\n",
    "        f'f1_{average}': f1\n",
    "    }\n",
    "\n",
    "    # For ROC AUC (multi-class or binary), we only have discrete predictions.\n",
    "    # We convert them to one-hot format for a 'one-vs-rest' approach in roc_auc_score.\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    try:\n",
    "        y_true_1hot = np.zeros((len(y_true), num_classes), dtype=int)\n",
    "        y_pred_1hot = np.zeros((len(y_pred), num_classes), dtype=int)\n",
    "\n",
    "        for i, lbl in enumerate(y_true):\n",
    "            y_true_1hot[i, lbl] = 1\n",
    "        for i, lbl in enumerate(y_pred):\n",
    "            y_pred_1hot[i, lbl] = 1\n",
    "\n",
    "        roc = roc_auc_score(\n",
    "            y_true_1hot,\n",
    "            y_pred_1hot,\n",
    "            multi_class='ovr',  # or 'ovo'\n",
    "            average=average\n",
    "        )\n",
    "        metrics_dict[f'roc_auc_{average}'] = roc\n",
    "\n",
    "    except ValueError:\n",
    "        # This can happen if there's only one class in y_true, etc.\n",
    "        metrics_dict[f'roc_auc_{average}'] = None\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def find_labels_with_min_count(labels, min_count=15):\n",
    "    \"\"\"\n",
    "    Find the positions of labels whose count is greater than or equal to `min_count`.\n",
    "\n",
    "    Args:\n",
    "        labels (list): A list of labels.\n",
    "        min_count (int): The minimum count to filter labels (default=15).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of labels that meet the count condition.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Find labels with count >= min_count\n",
    "    valid_labels = {label for label, count in label_counts.items() if count >= min_count}\n",
    "    \n",
    "    # Find indices of valid labels\n",
    "    indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    'PSRT',\n",
    "]\n",
    "\n",
    "csv_names = [\n",
    "    # 'Yau2020_record_processed',\n",
    "    'Yau2022_record_processed',\n",
    "    # 'NCBI_record_valid_nucleotide',\n",
    "    # 'NCBI_record_valid_count',\n",
    "]\n",
    "\n",
    "features_needed = [('facet', 0)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for encoder in encoders:\n",
    "        print(\"encoder \", encoder)\n",
    "        for data_name in csv_names:\n",
    "            print(\"data \", data_name)\n",
    "            for k in range(4,5):\n",
    "                print(\"k \", k)\n",
    "\n",
    "                for feature, max_dim in features_needed:\n",
    "                    print(feature)\n",
    "                    print(max_dim)\n",
    "                    path_to_data = f'data/{data_name}'\n",
    "                    df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "                    x_list = df['Accession (version)'].to_list()\n",
    "                    y_list = df['Family'].to_list()\n",
    "                    x_list, y_list = zip(*sorted(zip(x_list, y_list)))\n",
    "                    x_list = list(x_list)\n",
    "                    y_list = list(y_list)\n",
    "                    indices = find_labels_with_min_count(y_list, min_count=3)\n",
    "                    x_list = [x_list[i] for i in indices]\n",
    "                    y_list = [y_list[i] for i in indices]\n",
    "\n",
    "\n",
    "                    path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # for d in range(max_dim + 1):\n",
    "                    # print(d)\n",
    "                    distance_matrix = np.load(path_to_features+f'/k{k}_distance_{feature}{max_dim}.npy')\n",
    "                    subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\n",
    "                    # path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # # for d in range(max_dim + 1):\n",
    "                    # distance_matrix = np.load(path_to_features+f'/k{k}_distance.npy')\n",
    "                    # subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "                    le = LabelEncoder()\n",
    "                    y_int = le.fit_transform(y_list)  # shape (N,)\n",
    "                    y_np = np.array(y_int, dtype=np.int32)\n",
    "                    labels = y_np\n",
    "\n",
    "\n",
    "                    # 1-NN or k-NN using the distance matrix\n",
    "                    y_pred = leave_one_out_knn(subD, labels, n_neighbors=1)\n",
    "\n",
    "                    # Compute metrics\n",
    "                    metrics_result = compute_metrics(labels, y_pred, average='macro')\n",
    "\n",
    "                    # Print results\n",
    "                    print(\"Leave-One-Out k-NN Results (k={})\".format(k))\n",
    "                    for s, v in metrics_result.items():\n",
    "                        if v is not None:\n",
    "                            print(f\"{s}: {v:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"{s}: None\")\n",
    "                    print(\"===============================================\")\n",
    "                    print(\"===============================================\")\n",
    "                    print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder  PSRT\n",
      "data  NCBI_record_valid_nucleotide\n",
      "k  4\n",
      "facet\n",
      "0\n",
      "Leave-One-Out k-NN Results (k=4)\n",
      "accuracy: 0.8913\n",
      "balanced_accuracy: 0.7831\n",
      "precision_macro: 0.8321\n",
      "recall_macro: 0.7831\n",
      "f1_macro: 0.7962\n",
      "roc_auc_macro: 0.8913\n",
      "===============================================\n",
      "===============================================\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "leave_one_out_knn_eval_distance.py\n",
    "\n",
    "Script to perform a leave-one-out 1-NN classification from a precomputed NxN\n",
    "distance matrix, and then compute various metrics:\n",
    "- Accuracy\n",
    "- Balanced Accuracy\n",
    "- Precision, Recall, F1\n",
    "- ROC AUC (multi-class or binary)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def leave_one_out_knn(distance_matrix, labels, n_neighbors=1):\n",
    "    \"\"\"\n",
    "    Perform leave-one-out k-NN (here k=1 by default) classification using\n",
    "    a precomputed NxN distance matrix.\n",
    "\n",
    "    For each sample i, we:\n",
    "      - Look at distance_matrix[i], which contains distances to all other samples.\n",
    "      - Exclude itself by setting distance_matrix[i,i] = np.inf (if not already).\n",
    "      - Find the nearest neighbor(s).\n",
    "      - Predict the label of the chosen neighbor(s).\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): shape (N, N), distance_matrix[i,j] is the\n",
    "                                   distance between samples i and j.\n",
    "        labels (ndarray): shape (N,), integer or string labels.\n",
    "        n_neighbors (int): number of neighbors (default=1).\n",
    "\n",
    "    Returns:\n",
    "        y_pred (ndarray): predicted labels for each sample (length N)\n",
    "    \"\"\"\n",
    "    N = distance_matrix.shape[0]\n",
    "    y_pred = np.zeros(N, dtype=labels.dtype)\n",
    "\n",
    "    for i in range(N):\n",
    "        # distances from sample i to all others\n",
    "        dist = distance_matrix[i].copy()\n",
    "        # exclude itself\n",
    "        dist[i] = np.inf\n",
    "\n",
    "        # find the index/indices of the nearest neighbor(s)\n",
    "        nn_idx = np.argpartition(dist, n_neighbors)[:n_neighbors]\n",
    "        if n_neighbors == 1:\n",
    "            chosen_label = labels[nn_idx[0]]\n",
    "        else:\n",
    "            # If k>1, we can do majority vote among chosen neighbors\n",
    "            chosen_labels = labels[nn_idx]\n",
    "            chosen_label = np.bincount(chosen_labels).argmax()\n",
    "\n",
    "        y_pred[i] = chosen_label\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, average='weighted'):\n",
    "    \"\"\"\n",
    "    Compute various classification metrics.\n",
    "    For multi-class, use 'average' = 'macro' or 'weighted'.\n",
    "\n",
    "    Returns a dict of metrics:\n",
    "    - accuracy\n",
    "    - balanced_accuracy\n",
    "    - precision_{average}\n",
    "    - recall_{average}\n",
    "    - f1_{average}\n",
    "    - roc_auc_{average}  (if applicable)\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        f'precision_{average}': prec,\n",
    "        f'recall_{average}': rec,\n",
    "        f'f1_{average}': f1\n",
    "    }\n",
    "\n",
    "    # For ROC AUC (multi-class or binary), we only have discrete predictions.\n",
    "    # We convert them to one-hot format for a 'one-vs-rest' approach in roc_auc_score.\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    try:\n",
    "        y_true_1hot = np.zeros((len(y_true), num_classes), dtype=int)\n",
    "        y_pred_1hot = np.zeros((len(y_pred), num_classes), dtype=int)\n",
    "\n",
    "        for i, lbl in enumerate(y_true):\n",
    "            y_true_1hot[i, lbl] = 1\n",
    "        for i, lbl in enumerate(y_pred):\n",
    "            y_pred_1hot[i, lbl] = 1\n",
    "\n",
    "        roc = roc_auc_score(\n",
    "            y_true_1hot,\n",
    "            y_pred_1hot,\n",
    "            multi_class='ovr',  # or 'ovo'\n",
    "            average=average\n",
    "        )\n",
    "        metrics_dict[f'roc_auc_{average}'] = roc\n",
    "\n",
    "    except ValueError:\n",
    "        # This can happen if there's only one class in y_true, etc.\n",
    "        metrics_dict[f'roc_auc_{average}'] = None\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def find_labels_with_min_count(labels, min_count=15):\n",
    "    \"\"\"\n",
    "    Find the positions of labels whose count is greater than or equal to `min_count`.\n",
    "\n",
    "    Args:\n",
    "        labels (list): A list of labels.\n",
    "        min_count (int): The minimum count to filter labels (default=15).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of labels that meet the count condition.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Find labels with count >= min_count\n",
    "    valid_labels = {label for label, count in label_counts.items() if count >= min_count}\n",
    "    \n",
    "    # Find indices of valid labels\n",
    "    indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    'PSRT',\n",
    "]\n",
    "\n",
    "csv_names = [\n",
    "    # 'Yau2020_record_processed',\n",
    "    # 'Yau2022_record_processed',\n",
    "    'NCBI_record_valid_nucleotide',\n",
    "    # 'NCBI_record_valid_count',\n",
    "]\n",
    "\n",
    "features_needed = [('facet', 0)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for encoder in encoders:\n",
    "        print(\"encoder \", encoder)\n",
    "        for data_name in csv_names:\n",
    "            print(\"data \", data_name)\n",
    "            for k in range(4,5):\n",
    "                print(\"k \", k)\n",
    "\n",
    "                for feature, max_dim in features_needed:\n",
    "                    print(feature)\n",
    "                    print(max_dim)\n",
    "                    path_to_data = f'data/{data_name}'\n",
    "                    df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "                    x_list = df['Accession (version)'].to_list()\n",
    "                    y_list = df['Family'].to_list()\n",
    "                    x_list, y_list = zip(*sorted(zip(x_list, y_list)))\n",
    "                    x_list = list(x_list)\n",
    "                    y_list = list(y_list)\n",
    "                    indices = find_labels_with_min_count(y_list, min_count=3)\n",
    "                    x_list = [x_list[i] for i in indices]\n",
    "                    y_list = [y_list[i] for i in indices]\n",
    "\n",
    "\n",
    "                    path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # for d in range(max_dim + 1):\n",
    "                    # print(d)\n",
    "                    distance_matrix = np.load(path_to_features+f'/k{k}_distance_{feature}{max_dim}.npy')\n",
    "                    subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\n",
    "                    # path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # # for d in range(max_dim + 1):\n",
    "                    # distance_matrix = np.load(path_to_features+f'/k{k}_distance.npy')\n",
    "                    # subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "                    le = LabelEncoder()\n",
    "                    y_int = le.fit_transform(y_list)  # shape (N,)\n",
    "                    y_np = np.array(y_int, dtype=np.int32)\n",
    "                    labels = y_np\n",
    "\n",
    "\n",
    "                    # 1-NN or k-NN using the distance matrix\n",
    "                    y_pred = leave_one_out_knn(subD, labels, n_neighbors=1)\n",
    "\n",
    "                    # Compute metrics\n",
    "                    metrics_result = compute_metrics(labels, y_pred, average='macro')\n",
    "\n",
    "                    # Print results\n",
    "                    print(\"Leave-One-Out k-NN Results (k={})\".format(k))\n",
    "                    for s, v in metrics_result.items():\n",
    "                        if v is not None:\n",
    "                            print(f\"{s}: {v:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"{s}: None\")\n",
    "                    print(\"===============================================\")\n",
    "                    print(\"===============================================\")\n",
    "                    print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI 2024 All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder  PSRT\n",
      "data  NCBI_record_valid_count\n",
      "k  4\n",
      "facet\n",
      "0\n",
      "Leave-One-Out k-NN Results (k=4)\n",
      "accuracy: 0.8922\n",
      "balanced_accuracy: 0.7807\n",
      "precision_macro: 0.8243\n",
      "recall_macro: 0.7807\n",
      "f1_macro: 0.7936\n",
      "roc_auc_macro: 0.8901\n",
      "===============================================\n",
      "===============================================\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "leave_one_out_knn_eval_distance.py\n",
    "\n",
    "Script to perform a leave-one-out 1-NN classification from a precomputed NxN\n",
    "distance matrix, and then compute various metrics:\n",
    "- Accuracy\n",
    "- Balanced Accuracy\n",
    "- Precision, Recall, F1\n",
    "- ROC AUC (multi-class or binary)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def leave_one_out_knn(distance_matrix, labels, n_neighbors=1):\n",
    "    \"\"\"\n",
    "    Perform leave-one-out k-NN (here k=1 by default) classification using\n",
    "    a precomputed NxN distance matrix.\n",
    "\n",
    "    For each sample i, we:\n",
    "      - Look at distance_matrix[i], which contains distances to all other samples.\n",
    "      - Exclude itself by setting distance_matrix[i,i] = np.inf (if not already).\n",
    "      - Find the nearest neighbor(s).\n",
    "      - Predict the label of the chosen neighbor(s).\n",
    "\n",
    "    Args:\n",
    "        distance_matrix (ndarray): shape (N, N), distance_matrix[i,j] is the\n",
    "                                   distance between samples i and j.\n",
    "        labels (ndarray): shape (N,), integer or string labels.\n",
    "        n_neighbors (int): number of neighbors (default=1).\n",
    "\n",
    "    Returns:\n",
    "        y_pred (ndarray): predicted labels for each sample (length N)\n",
    "    \"\"\"\n",
    "    N = distance_matrix.shape[0]\n",
    "    y_pred = np.zeros(N, dtype=labels.dtype)\n",
    "\n",
    "    for i in range(N):\n",
    "        # distances from sample i to all others\n",
    "        dist = distance_matrix[i].copy()\n",
    "        # exclude itself\n",
    "        dist[i] = np.inf\n",
    "\n",
    "        # find the index/indices of the nearest neighbor(s)\n",
    "        nn_idx = np.argpartition(dist, n_neighbors)[:n_neighbors]\n",
    "        if n_neighbors == 1:\n",
    "            chosen_label = labels[nn_idx[0]]\n",
    "        else:\n",
    "            # If k>1, we can do majority vote among chosen neighbors\n",
    "            chosen_labels = labels[nn_idx]\n",
    "            chosen_label = np.bincount(chosen_labels).argmax()\n",
    "\n",
    "        y_pred[i] = chosen_label\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, average='weighted'):\n",
    "    \"\"\"\n",
    "    Compute various classification metrics.\n",
    "    For multi-class, use 'average' = 'macro' or 'weighted'.\n",
    "\n",
    "    Returns a dict of metrics:\n",
    "    - accuracy\n",
    "    - balanced_accuracy\n",
    "    - precision_{average}\n",
    "    - recall_{average}\n",
    "    - f1_{average}\n",
    "    - roc_auc_{average}  (if applicable)\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': bal_acc,\n",
    "        f'precision_{average}': prec,\n",
    "        f'recall_{average}': rec,\n",
    "        f'f1_{average}': f1\n",
    "    }\n",
    "\n",
    "    # For ROC AUC (multi-class or binary), we only have discrete predictions.\n",
    "    # We convert them to one-hot format for a 'one-vs-rest' approach in roc_auc_score.\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    try:\n",
    "        y_true_1hot = np.zeros((len(y_true), num_classes), dtype=int)\n",
    "        y_pred_1hot = np.zeros((len(y_pred), num_classes), dtype=int)\n",
    "\n",
    "        for i, lbl in enumerate(y_true):\n",
    "            y_true_1hot[i, lbl] = 1\n",
    "        for i, lbl in enumerate(y_pred):\n",
    "            y_pred_1hot[i, lbl] = 1\n",
    "\n",
    "        roc = roc_auc_score(\n",
    "            y_true_1hot,\n",
    "            y_pred_1hot,\n",
    "            multi_class='ovr',  # or 'ovo'\n",
    "            average=average\n",
    "        )\n",
    "        metrics_dict[f'roc_auc_{average}'] = roc\n",
    "\n",
    "    except ValueError:\n",
    "        # This can happen if there's only one class in y_true, etc.\n",
    "        metrics_dict[f'roc_auc_{average}'] = None\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def find_labels_with_min_count(labels, min_count=15):\n",
    "    \"\"\"\n",
    "    Find the positions of labels whose count is greater than or equal to `min_count`.\n",
    "\n",
    "    Args:\n",
    "        labels (list): A list of labels.\n",
    "        min_count (int): The minimum count to filter labels (default=15).\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of labels that meet the count condition.\n",
    "    \"\"\"\n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Find labels with count >= min_count\n",
    "    valid_labels = {label for label, count in label_counts.items() if count >= min_count}\n",
    "    \n",
    "    # Find indices of valid labels\n",
    "    indices = [i for i, label in enumerate(labels) if label in valid_labels]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "encoders = [\n",
    "    'PSRT',\n",
    "]\n",
    "\n",
    "csv_names = [\n",
    "    # 'Yau2020_record_processed',\n",
    "    # 'Yau2022_record_processed',\n",
    "    # 'NCBI_record_valid_nucleotide',\n",
    "    'NCBI_record_valid_count',\n",
    "]\n",
    "\n",
    "features_needed = [('facet', 0)]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for encoder in encoders:\n",
    "        print(\"encoder \", encoder)\n",
    "        for data_name in csv_names:\n",
    "            print(\"data \", data_name)\n",
    "            for k in range(4,5):\n",
    "                print(\"k \", k)\n",
    "\n",
    "                for feature, max_dim in features_needed:\n",
    "                    print(feature)\n",
    "                    print(max_dim)\n",
    "                    path_to_data = f'data/{data_name}'\n",
    "                    df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "                    x_list = df['Accession (version)'].to_list()\n",
    "                    y_list = df['Family'].to_list()\n",
    "                    x_list, y_list = zip(*sorted(zip(x_list, y_list)))\n",
    "                    x_list = list(x_list)\n",
    "                    y_list = list(y_list)\n",
    "                    indices = find_labels_with_min_count(y_list, min_count=3)\n",
    "                    x_list = [x_list[i] for i in indices]\n",
    "                    y_list = [y_list[i] for i in indices]\n",
    "\n",
    "\n",
    "                    path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # for d in range(max_dim + 1):\n",
    "                    # print(d)\n",
    "                    distance_matrix = np.load(path_to_features+f'/k{k}_distance_{feature}{max_dim}.npy')\n",
    "                    subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\n",
    "                    # path_to_features = f'distances/{encoder}/{data_name}'\n",
    "                    # # for d in range(max_dim + 1):\n",
    "                    # distance_matrix = np.load(path_to_features+f'/k{k}_distance.npy')\n",
    "                    # subD = distance_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "                    le = LabelEncoder()\n",
    "                    y_int = le.fit_transform(y_list)  # shape (N,)\n",
    "                    y_np = np.array(y_int, dtype=np.int32)\n",
    "                    labels = y_np\n",
    "\n",
    "\n",
    "                    # 1-NN or k-NN using the distance matrix\n",
    "                    y_pred = leave_one_out_knn(subD, labels, n_neighbors=1)\n",
    "\n",
    "                    # Compute metrics\n",
    "                    metrics_result = compute_metrics(labels, y_pred, average='macro')\n",
    "\n",
    "                    # Print results\n",
    "                    print(\"Leave-One-Out k-NN Results (k={})\".format(k))\n",
    "                    for s, v in metrics_result.items():\n",
    "                        if v is not None:\n",
    "                            print(f\"{s}: {v:.4f}\")\n",
    "                        else:\n",
    "                            print(f\"{s}: None\")\n",
    "                    print(\"===============================================\")\n",
    "                    print(\"===============================================\")\n",
    "                    print(\"===============================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
