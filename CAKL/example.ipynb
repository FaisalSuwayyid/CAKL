{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebolavirus_record\n",
      "4\n",
      "sample  0\n",
      "accession  FJ217161.1\n",
      "sample  1\n",
      "accession  KC545393.1\n",
      "sample  2\n",
      "accession  KC545395.1\n",
      "sample  3\n",
      "accession  KC545394.1\n",
      "sample  4\n",
      "accession  KC545396.1\n",
      "sample  5\n",
      "accession  FJ217162.1\n",
      "sample  6\n",
      "accession  AF522874.1\n",
      "sample  7\n",
      "accession  AB050936.1\n",
      "sample  8\n",
      "accession  JX477166.1\n",
      "sample  9\n",
      "accession  FJ621585.1\n",
      "sample  10\n",
      "accession  FJ621583.1\n",
      "sample  11\n",
      "accession  JX477165.1\n",
      "sample  12\n",
      "accession  FJ968794.1\n",
      "sample  13\n",
      "accession  KC242783.2\n",
      "sample  14\n",
      "accession  EU338380.1\n",
      "sample  15\n",
      "accession  AY729654.1\n",
      "sample  16\n",
      "accession  JN638998.1\n",
      "sample  17\n",
      "accession  KC545389.1\n",
      "sample  18\n",
      "accession  KC545390.1\n",
      "sample  19\n",
      "accession  KC545391.1\n",
      "sample  20\n",
      "accession  KC545392.1\n",
      "sample  21\n",
      "accession  KC589025.1\n",
      "sample  22\n",
      "accession  KC242801.1\n",
      "sample  23\n",
      "accession  NC_002549.1\n",
      "sample  24\n",
      "accession  KC242791.1\n",
      "sample  25\n",
      "accession  KC242792.1\n",
      "sample  26\n",
      "accession  KC242793.1\n",
      "sample  27\n",
      "accession  KC242794.1\n",
      "sample  28\n",
      "accession  AY354458.1\n",
      "sample  29\n",
      "accession  KC242796.1\n",
      "sample  30\n",
      "accession  KC242799.1\n",
      "sample  31\n",
      "accession  KC242784.1\n",
      "sample  32\n",
      "accession  KC242786.1\n",
      "sample  33\n",
      "accession  KC242787.1\n",
      "sample  34\n",
      "accession  KC242789.1\n",
      "sample  35\n",
      "accession  KC242785.1\n",
      "sample  36\n",
      "accession  KC242790.1\n",
      "sample  37\n",
      "accession  KC242788.1\n",
      "sample  38\n",
      "accession  KC242800.1\n",
      "sample  39\n",
      "accession  KM034555.1\n",
      "sample  40\n",
      "accession  KM034562.1\n",
      "sample  41\n",
      "accession  KM233039.1\n",
      "sample  42\n",
      "accession  KM034557.1\n",
      "sample  43\n",
      "accession  KM034560.1\n",
      "sample  44\n",
      "accession  KM233050.1\n",
      "sample  45\n",
      "accession  KM233053.1\n",
      "sample  46\n",
      "accession  KM233057.1\n",
      "sample  47\n",
      "accession  KM233063.1\n",
      "sample  48\n",
      "accession  KM233072.1\n",
      "sample  49\n",
      "accession  KM233110.1\n",
      "sample  50\n",
      "accession  KM233070.1\n",
      "sample  51\n",
      "accession  KM233099.1\n",
      "sample  52\n",
      "accession  KM233097.1\n",
      "sample  53\n",
      "accession  KM233109.1\n",
      "sample  54\n",
      "accession  KM233096.1\n",
      "sample  55\n",
      "accession  KM233103.1\n",
      "sample  56\n",
      "accession  KJ660346.2\n",
      "sample  57\n",
      "accession  KJ660347.2\n",
      "sample  58\n",
      "accession  KJ660348.2\n",
      "HEV_record\n",
      "4\n",
      "sample  0\n",
      "accession  M73218.1\n",
      "sample  1\n",
      "accession  D10330.1\n",
      "sample  2\n",
      "accession  X99441.1\n",
      "sample  3\n",
      "accession  AF076239.3\n",
      "sample  4\n",
      "accession  AF051830.1\n",
      "sample  5\n",
      "accession  AF185822.1\n",
      "sample  6\n",
      "accession  AF459438.1\n",
      "sample  7\n",
      "accession  D11092.1\n",
      "sample  8\n",
      "accession  L25595.1\n",
      "sample  9\n",
      "accession  L08816.1\n",
      "sample  10\n",
      "accession  D11093.1\n",
      "sample  11\n",
      "accession  M94177.1\n",
      "sample  12\n",
      "accession  M80581.1\n",
      "sample  13\n",
      "accession  X98292.1\n",
      "sample  14\n",
      "accession  AY230202.1\n",
      "sample  15\n",
      "accession  AY204877.1\n",
      "sample  16\n",
      "accession  M74506.1\n",
      "sample  17\n",
      "accession  AB089824.1\n",
      "sample  18\n",
      "accession  AB074918.2\n",
      "sample  19\n",
      "accession  AB074920.3\n",
      "sample  20\n",
      "accession  AF082843.1\n",
      "sample  21\n",
      "accession  AF060668.1\n",
      "sample  22\n",
      "accession  AF060669.1\n",
      "sample  23\n",
      "accession  AY115488.1\n",
      "sample  24\n",
      "accession  AB189070.1\n",
      "sample  25\n",
      "accession  AB189071.1\n",
      "sample  26\n",
      "accession  AB091394.1\n",
      "sample  27\n",
      "accession  AB189072.1\n",
      "sample  28\n",
      "accession  AP003430.1\n",
      "sample  29\n",
      "accession  AB189073.1\n",
      "sample  30\n",
      "accession  AB189074.1\n",
      "sample  31\n",
      "accession  AB189075.1\n",
      "sample  32\n",
      "accession  AB073912.1\n",
      "sample  33\n",
      "accession  AF455784.1\n",
      "sample  34\n",
      "accession  AB097812.1\n",
      "sample  35\n",
      "accession  AB099347.1\n",
      "sample  36\n",
      "accession  AB080575.1\n",
      "sample  37\n",
      "accession  AB074915.3\n",
      "sample  38\n",
      "accession  AB074917.3\n",
      "sample  39\n",
      "accession  AB161717.1\n",
      "sample  40\n",
      "accession  AB091395.1\n",
      "sample  41\n",
      "accession  AB200239.1\n",
      "sample  42\n",
      "accession  AB161718.1\n",
      "sample  43\n",
      "accession  AB161719.1\n",
      "sample  44\n",
      "accession  AB097811.1\n",
      "sample  45\n",
      "accession  AY594199.1\n",
      "sample  46\n",
      "accession  AJ272108.1\n",
      "sample  47\n",
      "accession  AB108537.1\n",
      "ebolavirus_record\n",
      "4\n",
      "facet 0\n",
      "d  0\n",
      "accessions  59\n",
      "matched 59\n",
      "HEV_record\n",
      "4\n",
      "facet 0\n",
      "d  0\n",
      "accessions  48\n",
      "matched 48\n",
      "Encoder: PSRT\n",
      "Dataset: ebolavirus_record\n",
      "k = 4\n",
      "  Feature: facet\n",
      "    Computing pairwise distances (metric: minkowski)...\n",
      "    [Saved] distances2/PSRT/ebolavirus_record/k4_distance_facet0.npy\n",
      "  --------------------------------------------------\n",
      "============================================================\n",
      "Dataset: HEV_record\n",
      "k = 4\n",
      "  Feature: facet\n",
      "    Computing pairwise distances (metric: minkowski)...\n",
      "    [Saved] distances2/PSRT/HEV_record/k4_distance_facet0.npy\n",
      "  --------------------------------------------------\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Standard libraries\n",
    "# -------------------------------\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "# -------------------------------\n",
    "# Scientific computing\n",
    "# -------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, to_tree, dendrogram\n",
    "\n",
    "# -------------------------------\n",
    "# Bioinformatics\n",
    "# -------------------------------\n",
    "from Bio import SeqIO\n",
    "\n",
    "# -------------------------------\n",
    "# Local modules\n",
    "# -------------------------------\n",
    "from psrt import *\n",
    "\n",
    "\n",
    "\n",
    "encoder = \"PSRT\"\n",
    "max_dimension = 0\n",
    "num = 0\n",
    "# k_chosen = 4\n",
    "\n",
    "features_needed = [('facet', 0)]\n",
    "# -------------------------------\n",
    "# Configuration parameters\n",
    "# -------------------------------\n",
    "\n",
    "alphabet = [\"A\", \"C\", \"G\", \"T\"]\n",
    "\n",
    "\n",
    "# # List of dataset base names (CSV and FASTA share the same prefix)\n",
    "\n",
    "csv_names = [\n",
    "    ('ebolavirus_record', 4),\n",
    "    ('HEV_record', 4),\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# Loop over datasets\n",
    "# -------------------------------\n",
    "for data_name, k_chosen in csv_names:\n",
    "    print(data_name)\n",
    "    path_to_data = f\"./data2/{data_name}\"\n",
    "\n",
    "    # Loop over k-mer sizes from 1 to 7\n",
    "    for k in range(k_chosen, k_chosen+1):\n",
    "        print(k)\n",
    "\n",
    "        # Load accessions from CSV\n",
    "        df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "        x = df['Accession (version)'].to_list()\n",
    "\n",
    "        # Parse FASTA sequences into a list of SeqRecord objects\n",
    "        file_path = f'{path_to_data}.fasta'\n",
    "        DNAs = list(SeqIO.parse(file_path, \"fasta\"))\n",
    "\n",
    "        # Create a dictionary: accession → SeqRecord\n",
    "        dna_dict = {record.id: record for record in DNAs}\n",
    "\n",
    "        # Match sequences with accessions from CSV, preserving order\n",
    "        ordered_dnas = [dna_dict[accession] for accession in x if accession in dna_dict]\n",
    "        dna_list = [str(record.seq) for record in ordered_dnas]\n",
    "\n",
    "        # Generate all possible k-mers of length k\n",
    "        kmers = generate_all_kmers(alphabet, k)\n",
    "\n",
    "        # Define filtration values (uniform grid scaled by 4^k)\n",
    "        specific_filtration = np.array([i * 4**k for i in range(0, num + 1)])\n",
    "        # specific_filtration = np.array([integ for integ in range(21)])\n",
    "\n",
    "        # -------------------------------\n",
    "        # Process each DNA sequence\n",
    "        # -------------------------------\n",
    "        for i in range(len(dna_list)):\n",
    "        # for i in range(a, min(b, len(dna_list))):\n",
    "            print(\"sample \", i)\n",
    "            print(\"accession \", ordered_dnas[i].id)\n",
    "            dna = dna_list[i]  # ← fix: define current sequence\n",
    "\n",
    "            # Initialize per-dimension feature accumulators\n",
    "            betti_result = (max_dimension + 1) * [None]\n",
    "            f_result = (max_dimension + 1) * [None]\n",
    "            h_result = (max_dimension + 2) * [None]\n",
    "            facet_result = (max_dimension + 1) * [None]\n",
    "\n",
    "            # -------------------------------\n",
    "            # Loop over all possible k-mers\n",
    "            # -------------------------------\n",
    "            for kmer in kmers:\n",
    "                points = occurrence(dna, kmer)\n",
    "\n",
    "                # Handle case where k-mer is absent\n",
    "                if len(points) == 0:\n",
    "                    zero_curve = np.zeros(specific_filtration.shape, dtype=float)\n",
    "                    betti_curves = (max_dimension + 1) * [zero_curve]\n",
    "                    f_curves = (max_dimension + 1) * [zero_curve]\n",
    "                    # h_curves = (max_dimension + 2) * [zero_curve]\n",
    "                    facet_curves = (max_dimension + 1) * [zero_curve]\n",
    "\n",
    "                    # Append zero curves to result arrays\n",
    "                    for d in range(max_dimension + 1):\n",
    "                        if betti_result[d] is None:\n",
    "                            betti_result[d] = betti_curves[d]\n",
    "                        else:\n",
    "                            betti_result[d] = np.vstack([betti_result[d], betti_curves[d]])\n",
    "\n",
    "                        if f_result[d] is None:\n",
    "                            f_result[d] = f_curves[d]\n",
    "                        else:\n",
    "                            f_result[d] = np.vstack([f_result[d], f_curves[d]])\n",
    "\n",
    "                        if facet_result[d] is None:\n",
    "                            facet_result[d] = facet_curves[d]\n",
    "                        else:\n",
    "                            facet_result[d] = np.vstack([facet_result[d], facet_curves[d]])\n",
    "\n",
    "                    # Append zero curves to result arrays\n",
    "                    for d in range(max_dimension + 2):\n",
    "                        if h_result[d] is None:\n",
    "                            h_result[d] = h_curves[d]\n",
    "                        else:\n",
    "                            h_result[d] = np.vstack([h_result[d], h_curves[d]])\n",
    "                    continue\n",
    "\n",
    "                # -------------------------------\n",
    "                # Compute PH features for current k-mer\n",
    "                # -------------------------------\n",
    "                points = np.array(points)[:, np.newaxis]  # Reshape to 2D\n",
    "\n",
    "                ph = PH(\n",
    "                    points,\n",
    "                    max_dimension=max_dimension,\n",
    "                    max_edge_length=2.0,\n",
    "                    specific_filtration=specific_filtration\n",
    "                )\n",
    "\n",
    "                alphas, betti_curves = ph.betti_curves()\n",
    "                f_curves = ph.compute_f_vector_curves()\n",
    "                h_curves = ph.compute_h_vector_curves()\n",
    "                facet_curves = ph.facet_curves()\n",
    "\n",
    "                # Stack curves per dimension\n",
    "                for d in range(max_dimension + 1):\n",
    "                    if betti_result[d] is None:\n",
    "                        betti_result[d] = betti_curves.get(d, np.zeros_like(alphas))\n",
    "                    else:\n",
    "                        betti_result[d] = np.vstack([betti_result[d], betti_curves.get(d, np.zeros_like(alphas))])\n",
    "\n",
    "                    if f_result[d] is None:\n",
    "                        f_result[d] = f_curves.get(d, np.zeros_like(alphas))\n",
    "                    else:\n",
    "                        f_result[d] = np.vstack([f_result[d], f_curves.get(d, np.zeros_like(alphas))])\n",
    "\n",
    "                    if facet_result[d] is None:\n",
    "                        facet_result[d] = facet_curves.get(d, np.zeros_like(alphas))\n",
    "                    else:\n",
    "                        facet_result[d] = np.vstack([facet_result[d], facet_curves.get(d, np.zeros_like(alphas))])\n",
    "\n",
    "                # Stack curves per dimension\n",
    "                for d in range(max_dimension + 2):\n",
    "                    if h_result[d] is None:\n",
    "                        h_result[d] = h_curves.get(d, np.zeros_like(alphas))\n",
    "                    else:\n",
    "                        h_result[d] = np.vstack([h_result[d], h_curves.get(d, np.zeros_like(alphas))])\n",
    "\n",
    "            # -------------------------------\n",
    "            # Save results to .npy files\n",
    "            # -------------------------------\n",
    "            save_path = f\"features2/{encoder}/{data_name}/{k}\"\n",
    "            os.makedirs(save_path, exist_ok=True)   # create folder path if missing\n",
    "            for d in range(max_dimension+1):\n",
    "                np.save(f\"./features2/{encoder}/{data_name}/{k}/{x[i]}_betti{d}.npy\", betti_result[d])\n",
    "                np.save(f\"./features2/{encoder}/{data_name}/{k}/{x[i]}_f{d}\", f_result[d])\n",
    "                np.save(f\"./features2/{encoder}/{data_name}/{k}/{x[i]}_facet{d}\", facet_result[d])\n",
    "\n",
    "            # -------------------------------\n",
    "            # Save results to .npy files\n",
    "            # -------------------------------\n",
    "            for d in range(max_dimension+2):\n",
    "                np.save(f\"./features2/{encoder}/{data_name}/{k}/{x[i]}_h{d}\", h_result[d])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stack_arrays_vertically(file_paths):\n",
    "    \"\"\"\n",
    "    Load NumPy arrays from given file paths and stack them vertically.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list of str): List of file paths to the NumPy arrays.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A single vertically stacked NumPy array.\n",
    "    \"\"\"\n",
    "    # Load each array and store it in a list\n",
    "    # arrays = [np.load(path) for path in file_paths]\n",
    "    arrays = [np.load(path).reshape(-1) for path in file_paths]\n",
    "    \n",
    "    # Stack the arrays vertically\n",
    "    stacked_array = np.vstack(arrays)\n",
    "    \n",
    "    return stacked_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data_name, k_chosen in csv_names:\n",
    "    print(data_name)\n",
    "    for k in range(k_chosen,k_chosen+1):\n",
    "        print(k)\n",
    "        path_to_features = f'features2/{encoder}/{data_name}/{k}'\n",
    "        path_to_data = f'data2/{data_name}'\n",
    "\n",
    "        # Read CSV file into DataFrame\n",
    "        df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "        # Load and sort accession IDs from metadata\n",
    "        accessions = df['Accession (version)'].to_list()\n",
    "\n",
    "        # Path to folder with feature files\n",
    "        folder_path = path_to_features\n",
    "\n",
    "        for considered_feature, considered_max_dimension in features_needed:\n",
    "            print(considered_feature, considered_max_dimension)\n",
    "            for d in range(considered_max_dimension+1):\n",
    "                print(\"d \", d)\n",
    "\n",
    "                # List all betti0 .npy files\n",
    "                file_names = [\n",
    "                    f for f in os.listdir(folder_path)\n",
    "                    if f.endswith('.npy') and f'{considered_feature}{d}' in f and os.path.isfile(os.path.join(folder_path, f))\n",
    "                ]\n",
    "\n",
    "                # Extract accession IDs from filenames (assumes format: accession_betti0.npy)\n",
    "                modified_names = [f.rsplit(f'_{considered_feature}{d}', 1)[0] for f in file_names]\n",
    "\n",
    "                # Filter accession IDs that have corresponding feature files\n",
    "                matched_accessions = [acc for acc in accessions if acc in modified_names]\n",
    "                print(\"accessions \", len(accessions))\n",
    "                print(\"matched\" , len(matched_accessions))\n",
    "\n",
    "                # Map accession → index in file_names\n",
    "                element_to_index = {name: idx for idx, name in enumerate(modified_names)}\n",
    "\n",
    "                # Build index list (skip any accessions not found in file_names)\n",
    "                indices = [element_to_index[acc] for acc in matched_accessions if acc in element_to_index]\n",
    "\n",
    "                # Sort file names to match accession order\n",
    "                file_names_sorted = [file_names[i] for i in indices]\n",
    "\n",
    "                # Full paths to sorted .npy files\n",
    "                file_paths = [os.path.join(folder_path, fname) for fname in file_names_sorted]\n",
    "\n",
    "                np.save(f'features2/{encoder}/{data_name}/k{k}_{considered_feature}{d}.npy', stack_arrays_vertically(file_paths))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Distance Computation Function\n",
    "# -------------------------------\n",
    "def compute_pairwise_distances(X, metric='euclidean', **kwargs):\n",
    "    \"\"\"\n",
    "    Compute pairwise distances for the dataset X.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): Dataset of shape (N, D), where N is the number of samples\n",
    "                     and D is the number of features.\n",
    "        metric (str): Distance metric to use. Supported metrics:\n",
    "                      'euclidean', 'manhattan', 'cosine', 'chebyshev',\n",
    "                      'minkowski', 'hamming', 'jaccard'.\n",
    "        kwargs: Additional arguments for specific metrics (e.g., p for Minkowski).\n",
    "\n",
    "    Returns:\n",
    "        distance_matrix (ndarray): Pairwise distance matrix of shape (N, N).\n",
    "    \"\"\"\n",
    "    # Compute condensed distance matrix using pdist\n",
    "    condensed_dist = pdist(X, metric=metric, **kwargs)\n",
    "    # Convert condensed form to square form\n",
    "    distance_matrix = squareform(condensed_dist)\n",
    "    return distance_matrix\n",
    "\n",
    "# -------------------------------\n",
    "# Main Script\n",
    "# -------------------------------\n",
    "\n",
    "print(f\"Encoder: {encoder}\")\n",
    "\n",
    "for data_name, k_chosen in csv_names:\n",
    "    print(f\"Dataset: {data_name}\")\n",
    "\n",
    "    for k in range(k_chosen, k_chosen + 1):\n",
    "        print(f\"k = {k}\")\n",
    "\n",
    "        for feature, max_dim in features_needed:\n",
    "            print(f\"  Feature: {feature}\")\n",
    "\n",
    "            for d in range(max_dim + 1):\n",
    "                input_path = f'features2/{encoder}/{data_name}/k{k}_{feature}{d}.npy'\n",
    "\n",
    "                if not os.path.exists(input_path):\n",
    "                    print(f\"    [Skipped] File not found: {input_path}\")\n",
    "                    continue\n",
    "\n",
    "                # Load and normalize data\n",
    "                X = np.load(input_path)\n",
    "                X = X/np.max(X)\n",
    "                # X = StandardScaler().fit_transform(X)\n",
    "\n",
    "                # Compute distance matrix\n",
    "                metric = 'minkowski'\n",
    "                p = 2\n",
    "                print(f\"    Computing pairwise distances (metric: {metric})...\")\n",
    "                dist_matrix = compute_pairwise_distances(X, metric=metric, p=p)\n",
    "                dist_matrix = dist_matrix/np.max(dist_matrix)\n",
    "                # dist_matrix /= np.max(dist_matrix)\n",
    "\n",
    "                # Save output\n",
    "                output_dir = f'distances2/{encoder}/{data_name}'\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_path = f'{output_dir}/k{k}_distance_{feature}{d}.npy'\n",
    "                np.save(output_path, dist_matrix)\n",
    "\n",
    "                print(f\"    [Saved] {output_path}\")\n",
    "\n",
    "            print(\"  \" + \"-\" * 50)\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_newick(node, leaf_names, newick='', parent_dist=0):\n",
    "    if node.is_leaf():\n",
    "        return \"%s:%.2f%s\" % (leaf_names[node.id], parent_dist - node.dist, newick)\n",
    "    else:\n",
    "        if len(newick) > 0:\n",
    "            newick = \"):%.2f%s\" % (parent_dist - node.dist, newick)\n",
    "        else:\n",
    "            newick = \");\"\n",
    "        newick = get_newick(node.get_left(), leaf_names, newick, node.dist)\n",
    "        newick = get_newick(node.get_right(), leaf_names, \",%s\" % newick, node.dist)\n",
    "        newick = \"(%s\" % newick\n",
    "        return newick\n",
    "\n",
    "\n",
    "\n",
    "def save_string_to_txt(file_name, content):\n",
    "    # Open the file in write mode ('w'). It creates the file if it doesn't exist.\n",
    "    with open(file_name, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "\n",
    "\n",
    "for data_name, k_chosen in csv_names:\n",
    "    print(\"data \", data_name)\n",
    "    for k in range(k_chosen, k_chosen + 1):\n",
    "        print(f\"k = {k}\")\n",
    "\n",
    "        for feature, max_dim in features_needed:\n",
    "            print(f\"  Feature: {feature}\")\n",
    "\n",
    "            for d in range(max_dim + 1):\n",
    "                path_to_distances = f'distances2/{encoder}/{data_name}'\n",
    "                distance_matrix = np.load(path_to_distances+f'/k{k}_distance_{feature}{d}.npy')\n",
    "\n",
    "                distance_matrix = distance_matrix/np.max(distance_matrix)\n",
    "                \n",
    "                path_to_data = f'data2/{data_name}'\n",
    "\n",
    "\n",
    "                # Read CSV file into DataFrame\n",
    "                df = pd.read_csv(f\"{path_to_data}.csv\")\n",
    "                # Load and sort accession IDs from metadata\n",
    "                # accessions = sorted(df['Accession (version)'].to_list())\n",
    "                accessions = df['Accession (version)'].to_list()\n",
    "                y = df['Name'].to_list()\n",
    "                y_list = y\n",
    "\n",
    "\n",
    "                linkage_metric = 'average'\n",
    "\n",
    "                Z = linkage(distance_matrix, linkage_metric)\n",
    "                # Z = sch.linkage(sch.distance.squareform(dist_matrix), method='average')\n",
    "                tree = to_tree(Z, rd=False)\n",
    "\n",
    "                unique_labels = list(set(y))\n",
    "                # Map each label to a color using a more readable palette\n",
    "                label_color_map = dict(zip(unique_labels, sns.color_palette(\"Set1\", len(unique_labels))))\n",
    "                leaf_colors = [label_color_map[label] for label in y]\n",
    "\n",
    "                # Create the dendrogram\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                dend = dendrogram(\n",
    "                    Z,\n",
    "                    labels=y,\n",
    "                    leaf_rotation=90,\n",
    "                    leaf_font_size=10\n",
    "                )\n",
    "\n",
    "                # Color the labels based on the mapping\n",
    "                ax = plt.gca()\n",
    "                x_labels = ax.get_xticklabels()\n",
    "                for x_label in x_labels:\n",
    "                    label_text = x_label.get_text()\n",
    "                    x_label.set_color(label_color_map[label_text])\n",
    "\n",
    "                # Add a legend for label colors\n",
    "                for lbl in unique_labels:\n",
    "                    plt.plot([], [], label=lbl, color=label_color_map[lbl])\n",
    "\n",
    "                plt.title(f'{encoder}_{data_name}_k{k}')\n",
    "                plt.xlabel('Sample index')\n",
    "                plt.ylabel('Distance')\n",
    "                # Save the figure\n",
    "                save_path = f\"trees2/{encoder}/{data_name}\"\n",
    "                os.makedirs(save_path, exist_ok=True)   # create folder path if missing\n",
    "                plt.savefig(f'trees2/{encoder}/{data_name}/k{k}_tree__{feature}{d}.pdf', dpi=300, bbox_inches='tight')\n",
    "                # plt.show()\n",
    "\n",
    "                # 6\n",
    "                # Convert to Newick\n",
    "                newick = get_newick(tree, y)\n",
    "                # print(\"Newick Format:\\n\", newick)\n",
    "\n",
    "                # 7\n",
    "                # save tree to txt file\n",
    "                save_string_to_txt(f'trees2/{encoder}/{data_name}/k{k}_tree_{feature}{d}.txt', newick)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
